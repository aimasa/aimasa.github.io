---
title: CNN论文笔记
copyright: true
date: 2019-11-20 21:12:23
categories:
- nlp自然语言处理
- CNN语言模型
tags:
- nlp
mathjax: true
---

阅读`CNN for sentence classification`的笔记，记录一下。

<!--more-->

对词向量的解释：

> Word vectors, wherein words are projected from a sparse, 1-of-V encoding (here V is the vocabulary size) onto a lower dimensional vector space via a hidden layer, are essentially feature extractors that encode semantic features of words in their dimensions.

本质是特征提取器，对词在其维度上的语义特征进行编码。



------

## 介绍

CNN应用领域挺广的，比如计算机视觉方面，音频识别方面，还有在自然语言处理方面。这里主要讲的是CNN在自然语言处理里面的运用以及一些知识点。

## 模型架构

在这篇论文里是一个简单的CNN模型，它只是一个简单的四层模型架构，分别有输入层、卷积层、池化层、全连接层。如下图所示：

<center>{% qnimg CNN论文笔记\filter.png%}</center>
【图截取自该论文，我就不自己画了】

### 输入层

这一层是将词语向量化，我的理解是把词语转换成0或1表示，现在用的比较多的词向量化工具是[word2Vec](https://code.google.com/p/word2vec/)。论文中用word2Vec对输入数据进行了两次word embeding，形成了两个通道（channel），一个进行卷积，一个通过反向传播进行微调一些参数（我目前理解的是调整权重和偏重，就是论文中的$f(w*x_{i:i+h-j} + b)$中的$w$和$b$，其中$w$是权重，它是随机初始化的，然后再根据loss的值进行反向传播微调，适应训练的参数类型，从而达到理想的分类效果，偏重就是$b$，以调整差值。根据反向传播的不断调整，再对结果进行筛选，得到最优的$w$和$b$值。



### 卷积层

这一层是用来提取输入参数的特征的，通过卷积，去提取特征。根据filter的高度，选择多少个词连接在一起与对应的filter进行卷积运算。

在卷积层中，filter对词向量做滑动取词计算，如果filter的高度$h$为$n$的话，那么和filter运算的分别是：

第一次运算：$x_{1:n}=x_{1}\oplus x_{2}…\oplus x_{n}$

第二次运算：$x_{2:n}=x_{2}\oplus x_{2}...\oplus x_{n+1}$

……

第x次运算：$x_{x:n}=x_{x}\oplus x_{x+1}…\oplus x_{n+x-1}$

注：如果$n+x-1 < len(输入参数)$，则继续运算，如果$n+x-1 = len(输入参数)$则结束运算，如果$n+x-1 > len(输入参数)$，那么会对其做padding处理，padding有两种处理方式，一种是VALID(不足舍弃)，另一种是SAME(不足补零)

【小声逼逼：一点都不想画图，因为感觉好麻烦哦，所以用公式表示啦

其中，$x_{i}$是指第$i$个词（word），其中  $x_{1:n}=x_{1}\oplus x_{2}…\oplus x_{n}$是$x_{1}$到$x_{n}$的词向量的连接（普通的连接哦）。

卷积运算的话就是用对应的权重和对应的拼接词汇向量相乘，再加上$b$偏移量，以此为参数放入选择好的激活函数中运算，从而得到对应的特征。就是：$f(w*x_{i:i+h-1}+b)$

很多博客都没有说过卷积运算是干么的，这里我对比着图像处理中的卷积运算讲讲咯。

图像处理中，用CNN进行分类计算，分类嘛，比如鸟类和鸟类的图片，只要图片上出现了鸟类，那就意味着这张图片我们可以把它归类为鸟类图片，但是，图片上出现鸟类的位置总不会一成不变，所以我们就要根据鸟类的特征去寻找图片上的鸟类，所以我们通过filter去一堆格子一堆格子的去搜寻，去比对，去寻找图片上可能出现的鸟类的局部特征，再通过分配不同的权重给搜寻结果，最后设置阈值，计算概率，以此判断这张图片是不是鸟类的图片。

语言处理也是一样的，它通过filter不同的高度，去寻找可能出现的词汇结构，再分配不同的权重，以此得到这个文本是不是这个类别的文本。【上下文相似的词其语义也相似------这是出自一篇经典论文】

### 池化层

这一层是用来挑选出各种filter的最明显的特征，一般都是用max_feature来挑选，是吧是吧，简单粗暴。然后如果还想继续加层的话，就可以在池化层后，接上卷积层，再接上池化层，以此循环迭代。

### 全连接层

这一层可以看成分类器，它的公式是：$y=w*z+b$，其中$z$当然是所有filter中的所有的最大特征值咯。最后这个$y$就被代入到激活函数里头$a = f(y)$。得出的值就是对应的属于不同类别的概率值，这些概率值就是全连接层的输出结果。

在计算出了概率值之后，根据预期结果（$\hat{y}$）和实际结果$a$计算出loss值（一般都是用$E = \frac{1}{2}\sum_{i}^{n}(\hat{y}-a)^{2}$这个公式计算出偏移值）这样就可以通过反向传播，去调整权重值和偏差值。

### 反向传参

还记得上面论文中说过，CNN在句子分类中，使用的是两个chennal，一个是静态chennal，一个是动态chennal，动态的 chennal是用来从全连接层反向传播调整参数的，静态的chennal是用来向传参，计算分类概率的。

反向传参的方法就是求导，从全连接层往输入层去反向推导调整权重值和偏差值，之前一直没能理解为什么反向求导时候怎么调整权重值，现在在这里把我目前的理解，讲一遍，然后如果理解错了，大家能够指正一下：

$w_{后} = w_{初} - \triangledown w$

$\triangledown w = \eta \ast \frac{\partial E}{\partial a}\ast \frac{\partial a}{\partial y}\ast \frac{\partial y}{\partial w}$

【这里只是意思意思的写那么几个公式，就是求偏导数，当 $\triangledown w$ 为0的时候，就意味着$w$的值已经被调整到预期（最佳）值了，就不会继续调整下去了，而为什么调整$w_{后}$要用 $w_{前}$ 去减去偏导值呢？因为偏导后，得到当下的斜率值，如果是正数，说明$w_{后}$的值需要减掉 $\triangledown w $ 值得到，如果是负数，就说明$w_{后}$的值需要加上 $\triangledown w $ 值得到。而里面的$\eta$是代表学习率，这个是自己设置的，是用来控制梯度下降的步长（这里可以看看吴恩达老师视频的梯度下降的这块），学习率太大，步长太长，就会很容易跳过让$w$能够等于最佳值的点，学习率太小，步长太小，就会下降速度太慢emmm，可以联想一下，数据不够，下降太慢==】

### 激活函数

分别有relu、tanh、sigmoi，这三种函数，一般，tanh、sigmoi会被用在全连接层，relu被用在卷积层。

激活函数的作用是把卷积后的结果压缩到某一个固定的范围，这样可以一直保持一层一层下去的数值范围是可控的。-------[摘自博客](https://www.jianshu.com/p/58168fec534d)

### 实验参数

$l_{2}$范式 = 3

filter window 的h分别有：3、4、5

每个高有100个feather map

batch= 50

### 总结

这就是一篇实验报告，CNN通过不同长度的filter去搜索句子中的特征，这样的话就避免了特征遗漏的现象，然后明显化重要特征，再通过全连接层分类。这样的话有个缺点就是无法联系上下文去判断词语在文中的意义，就是无法语义理解。因为中文一词多义，所以这样分类肯定会有误差的。所以nlp比图像处理难太多了。

啊，自己跟着网上的教程手动撸了CNN代码，对训练过程以及喂数据过程有了个大体的了解，因为电脑的性能问题，我不得已把数据集分批读取再丢进模型里面处理，不知道是不是这个原因，导致loss值为负值，而且准确度奇低，目前还在探索，等有了结果会再写一篇心得，标志着CNN的学习告一段落。

# 参考资料

[反向传播](https://zhuanlan.zhihu.com/p/32819991)

[CNN for sentence classification](https://www.aclweb.org/anthology/D14-1181.pdf)

