---
title: 机器学习之线性回归
copyright: true
date: 2019-06-21 09:34:22
categories:
- 算法
- 机器学习
- 线性回归算法
tags:
- 机器学习
- 算法
mathjax: true
---

在机器学习中，我听了吴恩达老师的几节课，一节是来介绍机器学习里面的监督学习，监督学习里面以回归分析和分类为代表，然后吴恩达老师介绍了回归算法，在此做笔记。

<!--more-->

会用到的表达符号：



- m：培训的样本的数量（prince in 1000’s有47列:m=47）


- x：输入变量，也称为‘特征’(Size in feet^2)


- y：输出变量，也叫‘目标变量’（prince in 1000’s）


- '#' ：训练样本的“个数”缩写


- $\theta_{i}\,=\,$：模型参数

# 回归算法

计算函数最小值：求导 = 0（因为求某数导是求的在当前这个数时候的切线斜率，只有在数值达到峰值时候，切线斜率才会为0）

回归算法分线性回归和非线性回归，这里先讨论线性回归。

## 线性回归

就是一条直线，有两个未知数（x和y）

公式：$h_{\theta }(x)=\theta _{0}x+\theta_{1}$

## 拟合

拟合就是把平面上的点,用一条光滑的曲线连接起来,因为这条曲线有无数种可能,就有了各种拟合方法.

拟合误差（即总残差）

## 绝对值的导数不存在的推导

函数可导必要条件是：在此点连续(也就是左连续必须和右连续相同)

这里是别人对绝对值的导数可能不存在而写出的证明：

<center>{%qnimg 机器学习之线性回归/proof.png%}</center>
## 最小二乘法

`最小二乘法`是一种数学优化技术，就是可以求到一些未知的数据，并且使这些求得的数据和实际数据间的误差的平方和为最小

## 损失函数（loss function）

就是真实值和理论值的偏差。

因为实际应用中会受到诸多因素的制约

## 效用函数

百度里说：效用函数通常是用来表示消费者在消费中所获得的效用与所消费的商品组合之间数量关系的函数，以衡量消费者从消费既定的商品组合中所获得满足的程度。

## 残差平方和

百度里解释的是：把数据点和它和回归直线上的预估点的差异数据称为残差，然后，所有的残差的平方和被称为残差平方和。它表示随机误差的效应，一组数据的残差平方和越小，它的拟合度也就越好。	

## 拟合

就是自己通过方程拟出来的曲线和已知的数据相吻合，这个过程叫做拟合

## 代价函数

在单变量线性回归函数（例）：$h_{\theta }(x)=\theta _{0}x+\theta_{1}$中的$\theta_{1}$和$\theta_{0}$(就是模型参数)这两个未知数可以有很多种组合，从而得出不同的线性函数，拟合出不同的曲线。

但是为了能够让拟合出来的曲线和已知数据尽最大程度吻合，挑选$\theta_{1}$和$\theta_{0}$很重要。

所以就有了求平均误差的公式：$J(\theta _{0} \,,\, \theta _{1} \, )=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta }(x^{(i)})-y^{(i)}))^{2}$

这个公式是用来衡量设置出来的$\theta_{1}$和$\theta_{0}$的值拟合的曲线和真实的数据的吻合度，所以这个公式得出来的结果越小越好。

> 其中的$\frac{1}{2}$这是在尝试减小平均误差

于是其中：minimize $J(\theta _{0} \,,\, \theta _{1} \, )$，就是这个线性函数的代价函数

# 参考资料

[拟合解释](https://baike.baidu.com/item/拟合)

[绝对值的导数不存在的原因](https://math.stackexchange.com/questions/991475/why-is-the-absolute-value-function-not-differentiable-at-x-0/991559)

[参考视频为吴恩达老师的机器学习的入门视频]