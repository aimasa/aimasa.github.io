<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>aimasa的个人博客</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://aimasa.github.io/"/>
  <updated>2020-01-07T06:45:03.907Z</updated>
  <id>http://aimasa.github.io/</id>
  
  <author>
    <name>ZHY</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>TensorFlow实现cnn踩坑点</title>
    <link href="http://aimasa.github.io/2019/12/25/TensorFlow%E5%AE%9E%E7%8E%B0cnn%E8%B8%A9%E5%9D%91%E7%82%B9/"/>
    <id>http://aimasa.github.io/2019/12/25/TensorFlow实现cnn踩坑点/</id>
    <published>2019-12-25T12:20:00.000Z</published>
    <updated>2020-01-07T06:45:03.907Z</updated>
    
    <content type="html"><![CDATA[<p>因为是新手，所以跟着网上的教程用<code>tensorflow</code>框架实现了<code>CNN</code>模型，并且开始了训练。在实现过程中经历了很多的坑，今天记录一下在实现过程中遇到过的坑。</p><p><a href="https://blog.csdn.net/White_Idiot/article/details/79253129" target="_blank" rel="noopener">教程博客</a></p><a id="more"></a><h1 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h1><p>我一直觉得数据处理只是简单的对数据进行数据加载，然后清洗（去掉停用词、标点符号、转义字符—比如<code>\n</code>以及空格等冗余数据）</p><p>其实，要做的处理并不止这些。</p><h2 id="标签处理"><a href="#标签处理" class="headerlink" title="标签处理"></a>标签处理</h2><p>在预处理中需要把不同文本对应的标签转换为one-hot类型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dense_to_one_hot</span><span class="params">(self, labels_dense, num_classes)</span>:</span>    </span><br><span class="line">    <span class="string">"""Convert class labels from scalars to one-hot vectors."""</span> </span><br><span class="line">    num_labels = labels_dense.shape[<span class="number">0</span>]    </span><br><span class="line">    index_offset = np.arange(num_labels) * num_classes    </span><br><span class="line">    labels_one_hot = np.zeros((num_labels, num_classes))    </span><br><span class="line">    temp = index_offset + labels_dense.ravel()    </span><br><span class="line">    labels_one_hot.flat[temp] = <span class="number">1</span>    </span><br><span class="line">    <span class="keyword">return</span> labels_one_hot</span><br></pre></td></tr></table></figure><p>在后面隐藏层输出结果与对应正确标签计算时候是需要的。</p><p>因为后面：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"output"</span>):    </span><br><span class="line">    W = tf.Variable(tf.truncated_normal([self.num_filters_total, self.num_classes]), name=<span class="string">"W"</span>)    </span><br><span class="line">    b = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[self.num_classes]), name=<span class="string">"b"</span>)       <span class="keyword">if</span> l2_reg_lambda:        </span><br><span class="line">        W_l2_loss = tf.contrib.layers.l2_regularizer(l2_reg_lambda)(W)    </span><br><span class="line">        tf.add_to_collection(<span class="string">"losses"</span>, W_l2_loss)    </span><br><span class="line">        self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=<span class="string">"scores"</span>)             <span class="comment"># (self.h_drop * W + b)    </span></span><br><span class="line">        self.predictions = tf.argmax(self.scores, <span class="number">1</span>, name=<span class="string">"predictions"</span>)           <span class="comment"># 找出分数中的最大值，就是预测值</span></span><br></pre></td></tr></table></figure><p>全连接输出时候，里面会通过<code>tf.nn.xw_plus_b()</code>计算得到的不同种类对应得到的<code>score</code>和以<code>one-hot</code>表示的标签下标值进行<code>softmax</code>计算误差，也就是<code>loss</code>值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">losses = tf.nn.softmax_cross_entropy_with_logits_v2(self.input_y, self.scores)</span><br><span class="line"><span class="comment"># softmax_cross_entropy_with_logits_v2第一个是对应标签的labels值，第二个logits值是预测分数，会自动转为对应的标签值进行计算</span></span><br></pre></td></tr></table></figure><h2 id="文本处理"><a href="#文本处理" class="headerlink" title="文本处理"></a>文本处理</h2><p>文本处理除掉上面说过的去停用词去标点符号去转义字符等，还需要把文本处理成机器可读取的模式，就是把文本变成数字。</p><p>我刚开始对这个的理解就是把文本转换为对应的词向量，再喂进模型，结果发现，并不是这样。</p><p>如果直接把词向量做词嵌入处理，这样会导致工程浩大（因为会有很多重复的词出现在文档里，在喂进模型前要把对应的词向量矩阵捣鼓出来，这样的话，就会消耗计算机大量的内存，降低速率。）所以我们会先把清洗后的文本转换成对应的文本下标的<code>ID</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vocab_processor =learn.preprocessing.VocabularyProcessor(normal_param.max_document_length)</span><br><span class="line">x = np.array(list(vocab_processor.fit_transform(x_texts)))</span><br></pre></td></tr></table></figure><p>【注：<code>x_texts</code>是已经经过清洗的数据，<code>x</code>就是<code>x_texts</code>对应的下标矩阵】</p><p>为了防止之后加载模型时候，测试文件不能转换为和训练集一样对应的下标<code>ID</code>，所以在训练集转换完对应的<code>ID</code>后，用</p><p>然后传入<code>cnn</code>模型中<code>word embeding</code>部分，找到对应的词向量，进行后续操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'/cpu:0'</span>), tf.name_scope(<span class="string">"embedding"</span>):    </span><br><span class="line">    self.W = tf.Variable(tf.random_uniform([self.vocab_size, self.embedding_size], <span class="number">-1.0</span>, <span class="number">1.0</span>))   </span><br><span class="line">    <span class="comment"># 随机化W，得到维度是self.vocab_size个self.embedding_size大小的矩阵，随机值在-1.0-1.0之间 </span></span><br><span class="line">    self.embedding_chars = tf.nn.embedding_lookup(self.W, self.input_x)    </span><br><span class="line">    <span class="comment"># 从id(索引)找到对应的One-hot encoding    </span></span><br><span class="line">    self.embedding_chars_expanded = tf.expand_dims(self.embedding_chars, <span class="number">-1</span>)    <span class="comment"># 增加维度</span></span><br></pre></td></tr></table></figure><p><code>W</code>相当于词袋模型，每个词都是个独立的个体，所以通过<code>tf.variable</code>设置随机变量，然后再根据前面处理好的下标过来找到不同的词对应的embeding。</p><p>因为是词袋模型，所以不好的地方在于，会丢失上下文的关系，造成准确度偏低的情况。</p><p>所以会有词预处理模型，比如<code>Word2vec</code>、最近特别热门的<code>Bert</code>这些模型，都会想办法获取词的上下文关系，得到对应的词向量，增加预测结果的准确性。</p><h1 id="模型搭建"><a href="#模型搭建" class="headerlink" title="模型搭建"></a>模型搭建</h1><p>在搭建<code>CNN</code>模型的过程中也碰到过一些问题，因为是新手，看着论文，也不知道如何下手去搭建这个模型，所以跟着教程才写了出来，在讨论碰到的问题之前，先总结一下经验。</p><p>先不提python语法问题，python是个很好的东西，对新手来说，有很多库，上手很快，是个很优美的语言。特别是在你了解到它的面向对象的特性的时候，它的优美实现会让你瞬间爱上它，反正我还没学会python。它是面向对象，我是面向百度谷歌。</p><p>接下来要说的是tensorflow框架了，TensorFlow框架是个坑，它升级到2.0版本，和1.0版本有很多不兼容的地方，但因为是新出的2.0，普及性还不是很广，出了问题翻遍它官方的issue都不一定能找到答案，所以我配合我的cuda版本装了TensorFlow1.13.1。其实这个因果关系不是特别紧密，我只是想吐槽它升级版本顺便改了改语法，让我初学者翻了一下午的资料，才发现为啥教程上是这样写没问题，我这样写就有问题了。</p><p>回到正文。</p><p>在用代码搭建<code>CNN</code>模型中，作者用了<code>tf.name_scope(&quot;&quot;)</code>把这个实现分成了词嵌入、卷积-池化、dropout、输出、loss值计算、准确值计算这六个部分</p><p><code>tf.name_scope(&quot;&quot;)</code>：它给我感觉就是把复杂的网络分开来，变成几个小的模块，不会因为网络复杂了，而数据混乱没有条理，导致出错。实际上和我想的差不多吧。</p><h2 id="loss绝对值增大"><a href="#loss绝对值增大" class="headerlink" title="loss绝对值增大"></a>loss绝对值增大</h2><p>当时loss值的绝对值越来越大，并没有收敛的趋势，我找了很久的问题，在学长的帮助下才发现出在这里：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">losses = tf.nn.softmax_cross_entropy_with_logits_v2(self.input_y, self.scores)</span><br></pre></td></tr></table></figure><p>这是第一个坑， softmax_cross_entropy_with_logits_v2第一个是对应标签的labels值，第二个logits值是预测分数，会自动转为对应的标签值进行计算出相应的loss值。</p><p>讲到这里，不得不提及一下<code>softmax</code>是什么了，<a href="https://zhuanlan.zhihu.com/p/53794191" target="_blank" rel="noopener">参考博客</a>博客里面给出了相关公式，是将每个类别给出的分数通过<code>softmax</code>进行计算，把分值均转换为正数，通过$S<em>{i} = \frac{e^{i}}{\sum</em>{j}e^{j}}$这个公式，把分值最高的值凸显出来，同时将输出结果映射到(0,1)之间，转换成概率。</p><p>loss值是由损失函数算出来的，这里使用交叉熵函数作为我们的损失函数。这里有交叉熵函数的公式。<a href="https://blog.csdn.net/QW_sunny/article/details/72885403" target="_blank" rel="noopener">参考博客</a></p><h2 id="对tf运行的理解"><a href="#对tf运行的理解" class="headerlink" title="对tf运行的理解"></a>对tf运行的理解</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">self.input_x = tf.placeholder(tf.int32, [<span class="keyword">None</span>, sequence_length], name=<span class="string">"input_x"</span>)</span><br><span class="line">self.input_y = tf.placeholder(tf.float32, [<span class="keyword">None</span>, num_classes], name=<span class="string">"input_y"</span>)</span><br></pre></td></tr></table></figure><p><code>tf</code>和平常代码不一样，日常代码都是先赋值，再进行运算，而<code>tf</code>是先定义运算，再进行初始化赋值，就像上面这两行代码一样，这两行代码是预定义了<code>input_x</code>和<code>input_y</code>的值，然后再在后面</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">step, summaries, loss, accuracy = sess.run([global_step, dev_summary_op, cnn_init.loss, cnn_init.accuracy], feed_dic)</span><br></pre></td></tr></table></figure><p>把定义好<code>feed_dic</code>的值，喂进初始化后的<code>cnn</code>模型中，再返回<code>[global_step, dev_summary_op, cnn_init.loss, cnn_init.accuracy]</code>里面的值（<code>cnn_init</code>就是初始化后的<code>CNN</code>模型）</p><p><code>global_step, dev_summary_op</code>和后面的值不一样，不是从模型里面输出的值，</p><h2 id="模型存取"><a href="#模型存取" class="headerlink" title="模型存取"></a>模型存取</h2><p>在程序的运行中，会因为各种问题导致程序突然中止，所以我想要运行到一定时间就自动保存一个正在学习的数据模型，再次运行时候，如果已经有保存了的模型，就加载出来。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">checkpoint_dir = os.path.abspath(os.path.join(out_dir, <span class="string">"checkpoint"</span>))</span><br><span class="line">checkpoint_prefix = os.path.join(checkpoint_dir, <span class="string">"model"</span>)</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(checkpoint_dir):    </span><br><span class="line">    os.makedirs(checkpoint_dir)</span><br><span class="line">saver = tf.train.Saver(tf.global_variables())ckpt = tf.train.latest_checkpoint(checkpoint_dir)</span><br><span class="line"><span class="keyword">if</span> ckpt:    </span><br><span class="line">    saver.restore(sess, ckpt)    </span><br><span class="line">    print(<span class="string">"CNN restore from the checkpoint &#123;0&#125;"</span>.format(ckpt))</span><br></pre></td></tr></table></figure><p>继续训练后，运行了好几遍，发现并没有办法加载已有模型继续训练，究其原因，发现，问题出在<code>session</code>初始化上面。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sess.run(tf.global_variables_initializer())</span><br></pre></td></tr></table></figure><p><a href="https://www.jianshu.com/p/bebcdfb74fb1" target="_blank" rel="noopener">参考博文</a></p><p>以此初始化全局变量</p><h1 id="电脑性能限制"><a href="#电脑性能限制" class="headerlink" title="电脑性能限制"></a>电脑性能限制</h1><p>因为实验室不是专门做深度学习这块，所以没有较好的设备去完成实验，目前实验设备有：</p><ol><li><p>CPU</p><pre><code>  Intel(R) Core(TM) i7-7700 CPU @ 3.60GHz  基准速度:    3.60 GHz  插槽:    1  内核:    4  逻辑处理器:    8</code></pre></li><li><p>GPU 1块</p><pre><code>NVIDIA GeForce RTX 2060 SUPER   专用 GPU 内存    8.0 GB   共享 GPU 内存    8.0 GB   GPU 内存    16.0 GB</code></pre></li><li><p>内存</p><pre><code>16.0 GB</code></pre></li><li><p>磁盘 1 (D: F: G: E:)</p><pre><code>WDC WD10EZEX-21M2NA0   容量:    932 GB   已格式化:    932 GB   系统磁盘:    否   页面文件:    是   读取速度    199 KB/秒   写入速度    41.0 KB/秒   活动时间    56%   平均响应时间    6.2 毫秒</code></pre></li></ol><p>目前硬件上面临的有几个困难：</p><ol><li>GPU利用率过低，不知道如何优化</li><li>读取文档的速度过慢，优化方法下面介绍</li></ol><p>其实总结一下就是，硬件设备性能低，没法加快程序运行速度。</p><h1 id="优化运行方案"><a href="#优化运行方案" class="headerlink" title="优化运行方案"></a>优化运行方案</h1><p>因为内存的不足，所以在学长的建议下，我把我需要输入的数据进行分段处理，这样一来能快速看到效果，二来被打断的话，不需要重新加载那么庞大的数据继续跑，三来我电脑性能限制，我没法一次性跑那么多数据。其实第三点才是最重要的。</p><p>这就要来说说<code>python</code>中<code>yield</code>的语法了。</p><p><code>yield</code>相当于<code>return</code>，不同的是，它不会结束程序的运行，而是让程序在经过yield的处理后，回到开始的循环，继续向下执行。可能这样干说不太能很好的表达，所以我写一段。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_iter</span><span class="params">(a, b, num_foreach)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_foreach):</span><br><span class="line">        c = a + b</span><br><span class="line">        <span class="keyword">yield</span> c</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    batch = batch_iter(<span class="number">1</span>,<span class="number">2</span>，<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">for</span> b <span class="keyword">in</span> batch:</span><br><span class="line">        t = b</span><br><span class="line">        print(t)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">3</span><br><span class="line">3</span><br><span class="line">3</span><br></pre></td></tr></table></figure><p>就是迭代器，里面<code>batch_iter</code>这个方法就是迭代时候会使用的方法，然后迭代完<code>c</code>就是输出的值。</p><p>根据这个语法的特性，我改进了一下代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">batches = process_data_init.batch_iter(normal_param.batch_size, normal_param.num_epochs)</span><br><span class="line"><span class="keyword">for</span> batch, dev_data, dev_label, is_save <span class="keyword">in</span> batches:    </span><br><span class="line">    x_batch, y_batch = zip(*batch)    <span class="comment"># print("y_batch", y_batch) </span></span><br><span class="line">    train_step(x_batch, y_batch, train_summary_write)    </span><br><span class="line">    current_step = tf.train.global_step(sess, global_step)    </span><br><span class="line">    <span class="keyword">if</span> current_step % normal_param.evaluate_every == <span class="number">0</span>:        </span><br><span class="line">        print(<span class="string">"\nEvaluation:"</span>)        </span><br><span class="line">        dev_step(dev_data, dev_label, writer=dev_summary_writer)</span><br></pre></td></tr></table></figure><p>把数据集的读取改到迭代时候读取。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_iter</span><span class="params">(self, batch_size, num_epochs, shuffle=True)</span>:</span>    </span><br><span class="line">    <span class="string">'''迭代器'''</span>    </span><br><span class="line">    <span class="comment"># num = 1    # data = np.array(data)    # data_size = len(data)    # num_batches_per_epoch = int((data_size - 1) / batch_size) + 1    </span></span><br><span class="line">    echo_part_num = len(self.all_text_path) // normal_param.num_database    </span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):        </span><br><span class="line">        print(<span class="string">"epoch:"</span>, epoch, <span class="string">"/"</span>, num_epochs)        </span><br><span class="line">        <span class="keyword">for</span> part_n <span class="keyword">in</span> range(normal_param.num_database):            </span><br><span class="line">            is_save = <span class="keyword">False</span>            </span><br><span class="line">            train_data, train_label, dev_data, dev_label, vocal_size_train = self.deal_data(part=echo_part_num,n_part=part_n)            </span><br><span class="line">            data = list(zip(train_data, train_label))            </span><br><span class="line">            data = np.array(data)            </span><br><span class="line">            data_size = len(data)            </span><br><span class="line">            num_batches_per_epoch = int((data_size - <span class="number">1</span>) / batch_size) + <span class="number">1</span>            </span><br><span class="line">            <span class="keyword">if</span> shuffle:                </span><br><span class="line">                shuffle_indices = np.random.permutation(np.arange(data_size))         </span><br><span class="line">                shuffle_data = data[shuffle_indices]            </span><br><span class="line">             <span class="keyword">else</span>:                </span><br><span class="line">                shuffle_data = data            </span><br><span class="line">             <span class="keyword">for</span> batch_num <span class="keyword">in</span> range(num_batches_per_epoch):                </span><br><span class="line">                 start_idx = batch_num * batch_size                </span><br><span class="line">                 end_idx = min((batch_num + <span class="number">1</span>) * batch_size, data_size)                </span><br><span class="line">                 <span class="keyword">if</span> batch_num + <span class="number">1</span> == num_batches_per_epoch:                    </span><br><span class="line">                    is_save = <span class="keyword">True</span>                </span><br><span class="line">                    <span class="keyword">yield</span> shuffle_data[start_idx:end_idx], dev_data，dev_label,is_save</span><br></pre></td></tr></table></figure><h1 id="调参"><a href="#调参" class="headerlink" title="调参"></a>调参</h1><p>根据数据集大小以及过拟合情况，适当调整了batch_size的大小和学习率的大小、dropout的大小以及迭代的次数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">num_epochs = <span class="number">50</span></span><br><span class="line"><span class="comment">#（视情况定，验证集loss值上升结束后开始下降说明学习已经结束）</span></span><br><span class="line">dropout_keep_prob = <span class="number">0.5</span></span><br><span class="line"><span class="comment">#学习率设置：1e-3</span></span><br><span class="line">optimizer = tf.train.AdamOptimizer(<span class="number">1e-3</span>)</span><br></pre></td></tr></table></figure><h1 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h1><p>数据集我用的是THUCNew1,里面有14个class，将前三个class抽取出来，并且每个类别中的数据以6:1的比例抽取，所以实验数据集有10,788个文件数据，实验中使用TensorBoard可视化数据，结果如下：</p><p>神经网络流程图可视化</p><p><center><img src="http://pictures.aimasa.club/static/images/TensorFlow实现cnn踩坑点\sc.png"></center><br>验证集的loss曲线图：</p><p><center><img src="http://pictures.aimasa.club/static/images/TensorFlow实现cnn踩坑点\dev-loss.png"></center><br>验证集的acc的曲线：</p><p><center><img src="http://pictures.aimasa.club/static/images/TensorFlow实现cnn踩坑点\dev-acc.png"></center><br>训练集的acc曲线：</p><p><center><img src="http://pictures.aimasa.club/static/images/TensorFlow实现cnn踩坑点\train-acc.png"></center><br>训练集的loss曲线：</p><p><center><img src="http://pictures.aimasa.club/static/images/TensorFlow实现cnn踩坑点\train-loss.png"></center></p><h1 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h1><p><a href="https://github.com/aimasa/CNN_classify" target="_blank" rel="noopener">代码实现</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;因为是新手，所以跟着网上的教程用&lt;code&gt;tensorflow&lt;/code&gt;框架实现了&lt;code&gt;CNN&lt;/code&gt;模型，并且开始了训练。在实现过程中经历了很多的坑，今天记录一下在实现过程中遇到过的坑。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/White_Idiot/article/details/79253129&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;教程博客&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="nlp自然语言处理" scheme="http://aimasa.github.io/categories/nlp%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="CNN语言模型" scheme="http://aimasa.github.io/categories/nlp%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/CNN%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"/>
    
    
      <category term="nlp" scheme="http://aimasa.github.io/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>手写字体识别走过的坑</title>
    <link href="http://aimasa.github.io/2019/12/04/%E6%89%8B%E5%86%99%E5%AD%97%E4%BD%93%E8%AF%86%E5%88%AB%E8%B5%B0%E8%BF%87%E7%9A%84%E5%9D%91/"/>
    <id>http://aimasa.github.io/2019/12/04/手写字体识别走过的坑/</id>
    <published>2019-12-04T07:18:35.000Z</published>
    <updated>2019-12-27T13:11:31.724Z</updated>
    
    <content type="html"><![CDATA[<p>运行的代码是：<a href="https://github.com/HuiyanWen/bysj_hit" target="_blank" rel="noopener">哈工大大佬毕设代码</a></p><p>因为数据集他们没放出来，所以我选用的是中科院的手写数据集，特别有名的那个HWDB：<a href="http://www.nlpr.ia.ac.cn/databases/download/feature_data/HWDB1.1trn_gnt.zip" target="_blank" rel="noopener">数据集trn</a>，<a href="http://www.nlpr.ia.ac.cn/databases/download/feature_data/HWDB1.1tst_gnt.zip" target="_blank" rel="noopener">数据集tst</a></p><p>其实这里也算是记录用<code>TensorFlow</code>实现图片处理经过的坑。名字而已，无需计较</p><a id="more"></a><h1 id="开门第一坑：缺少验证集"><a href="#开门第一坑：缺少验证集" class="headerlink" title="开门第一坑：缺少验证集"></a>开门第一坑：缺少验证集</h1><p>自己写了个脚本，以3:1的比例从trn中抽取出验证集。【这些数据是要处理一下转换为图片的，网上很多教程，去搜就行了】因为很简单，所以不多说，进入第二坑</p><h1 id="开门第二坑：tf-data-TFRecordDataset-的使用"><a href="#开门第二坑：tf-data-TFRecordDataset-的使用" class="headerlink" title="开门第二坑：tf.data.TFRecordDataset()的使用"></a>开门第二坑：tf.data.TFRecordDataset()的使用</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.data.TFRecordDataset(filename)</span><br></pre></td></tr></table></figure><p>我傻了吧唧的以为，这个可以直接把地址列表输入进去，它会自动按地址去读取那些对应的图片，事实证明我错的离谱……这个只会读取存有TFRecord文件地址的列表……所以才会报错：record被损坏……</p><p>这个<code>TFRecordDataset</code>里面的<code>filename</code>只能是<code>TFRecord</code>类型文件的地址==。</p><p>所以现在先进行数据预处理。</p><p>TFRecord是一种将图像数据和标签放在一起的二进制文件，适合以串行的方式读取大批量数据，虽然它的内部格式复杂，但是它可以很好地利用内存，方便地复制和移动，更符合TensorFlow执行引擎的方式。<a href="https://blog.csdn.net/JinbaoSite/article/details/75194226" target="_blank" rel="noopener">该句来源</a></p><p><a href="https://blog.csdn.net/hujiameihuxu/article/details/79944366" target="_blank" rel="noopener">代码参考博客</a></p><p>关于代码中的<code>tf.python_io.TFRecordWriter(path, options=None)</code>，括号中间的<code>path</code>是填写转换好的内容的存储地址的,<code>option</code>是用来定义<code>TFRecord</code>文件保存的压缩格式的，是<code>TFRecordOptions</code>对象。[注意哦，TensorFlow的v2.0版本和v2.0以下版本的语法不同哟~反正我觉得TensorFlowv2.0的bug有点多，不敢用。而且我看了一下别人提交的push，好多是修改语法的……emmm还是等它稳定点，再试着用用吧。]</p><p>将普通图片数据转换成<code>TFRecord</code>格式数据需要注意的一点就是：读取<code>TFRecord</code>文件的时候，读取字典的<code>key</code>是要根据转换成<code>TFRecord</code>格式时候的字典的<code>key</code>对应上的。</p><p>【==我对tfrecord的理解有问题，需要修改==</p><p>好，接着进入第三坑。【暂缓</p><h1 id="对TensorFlow不了解的坑"><a href="#对TensorFlow不了解的坑" class="headerlink" title="对TensorFlow不了解的坑"></a>对TensorFlow不了解的坑</h1><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y = tf.nn.dropout(x, dropout)</span><br></pre></td></tr></table></figure><p>其中</p><p>x是输入的，需要dropout进行概率屏蔽的矩阵</p><p>dropout是概率，需要屏蔽哪些</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;运行的代码是：&lt;a href=&quot;https://github.com/HuiyanWen/bysj_hit&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;哈工大大佬毕设代码&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;因为数据集他们没放出来，所以我选用的是中科院的手写数据集，特别有名的那个HWDB：&lt;a href=&quot;http://www.nlpr.ia.ac.cn/databases/download/feature_data/HWDB1.1trn_gnt.zip&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;数据集trn&lt;/a&gt;，&lt;a href=&quot;http://www.nlpr.ia.ac.cn/databases/download/feature_data/HWDB1.1tst_gnt.zip&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;数据集tst&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;其实这里也算是记录用&lt;code&gt;TensorFlow&lt;/code&gt;实现图片处理经过的坑。名字而已，无需计较&lt;/p&gt;
    
    </summary>
    
      <category term="图像处理" scheme="http://aimasa.github.io/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
      <category term="手写识别" scheme="http://aimasa.github.io/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/%E6%89%8B%E5%86%99%E8%AF%86%E5%88%AB/"/>
    
    
      <category term="图像处理" scheme="http://aimasa.github.io/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>CNN论文笔记</title>
    <link href="http://aimasa.github.io/2019/11/20/CNN%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    <id>http://aimasa.github.io/2019/11/20/CNN论文笔记/</id>
    <published>2019-11-20T13:12:23.000Z</published>
    <updated>2020-01-06T12:52:18.293Z</updated>
    
    <content type="html"><![CDATA[<p>阅读<code>CNN for sentence classification</code>的笔记，记录一下。</p><a id="more"></a><p>对词向量的解释：</p><blockquote><p>Word vectors, wherein words are projected from a sparse, 1-of-V encoding (here V is the vocabulary size) onto a lower dimensional vector space via a hidden layer, are essentially feature extractors that encode semantic features of words in their dimensions.</p></blockquote><p>本质是特征提取器，对词在其维度上的语义特征进行编码。</p><hr><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>CNN应用领域挺广的，比如计算机视觉方面，音频识别方面，还有在自然语言处理方面。这里主要讲的是CNN在自然语言处理里面的运用以及一些知识点。</p><h2 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h2><p>在这篇论文里是一个简单的CNN模型，它只是一个简单的四层模型架构，分别有输入层、卷积层、池化层、全连接层。如下图所示：</p><p><center><img src="http://pictures.aimasa.club/static/images/CNN论文笔记\filter.png"></center><br>【图截取自该论文，我就不自己画了】</p><h3 id="输入层"><a href="#输入层" class="headerlink" title="输入层"></a>输入层</h3><p>这一层是将词语向量化，我的理解是把词语转换成0或1表示，现在用的比较多的词向量化工具是<a href="https://code.google.com/p/word2vec/" target="_blank" rel="noopener">word2Vec</a>。论文中用word2Vec对输入数据进行了两次word embeding，形成了两个通道（channel），一个进行卷积，一个通过反向传播进行微调一些参数（我目前理解的是调整权重和偏重，就是论文中的$f(w*x_{i:i+h-j} + b)$中的$w$和$b$，其中$w$是权重，它是随机初始化的，然后再根据loss的值进行反向传播微调，适应训练的参数类型，从而达到理想的分类效果，偏重就是$b$，以调整差值。根据反向传播的不断调整，再对结果进行筛选，得到最优的$w$和$b$值。</p><h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>这一层是用来提取输入参数的特征的，通过卷积，去提取特征。根据filter的高度，选择多少个词连接在一起与对应的filter进行卷积运算。</p><p>在卷积层中，filter对词向量做滑动取词计算，如果filter的高度$h$为$n$的话，那么和filter运算的分别是：</p><p>第一次运算：$x_{1:n}=x<em>{1}\oplus x\</em>{2}…\oplus x_{n}$</p><p>第二次运算：$x_{2:n}=x_{2}\oplus x_{2}…\oplus x_{n+1}$</p><p>……</p><p>第x次运算：$x_{x:n}=x_{x}\oplus x_{x+1}…\oplus x_{n+x-1}$</p><p>注：如果$n+x-1 &lt; len(输入参数)$，则继续运算，如果$n+x-1 = len(输入参数)$则结束运算，如果$n+x-1 &gt; len(输入参数)$，那么会对其做padding处理，padding有两种处理方式，一种是VALID(不足舍弃)，另一种是SAME(不足补零)</p><p>【小声逼逼：一点都不想画图，因为感觉好麻烦哦，所以用公式表示啦</p><p>其中，$x<em>{i}$是指第$i$个词（word），其中  $x\</em>{1:n}=x_{1}\oplus x_{2}…\oplus x_{n}$是$x<em>{1}$到$x</em>{n}$的词向量的连接（普通的连接哦）。</p><p>卷积运算的话就是用对应的权重和对应的拼接词汇向量相乘，再加上$b$偏移量，以此为参数放入选择好的激活函数中运算，从而得到对应的特征。就是：$f(w*x_{i:i+h-1}+b)$</p><p>很多博客都没有说过卷积运算是干么的，这里我对比着图像处理中的卷积运算讲讲咯。</p><p>图像处理中，用CNN进行分类计算，分类嘛，比如鸟类和鸟类的图片，只要图片上出现了鸟类，那就意味着这张图片我们可以把它归类为鸟类图片，但是，图片上出现鸟类的位置总不会一成不变，所以我们就要根据鸟类的特征去寻找图片上的鸟类，所以我们通过filter去一堆格子一堆格子的去搜寻，去比对，去寻找图片上可能出现的鸟类的局部特征，再通过分配不同的权重给搜寻结果，最后设置阈值，计算概率，以此判断这张图片是不是鸟类的图片。</p><p>语言处理也是一样的，它通过filter不同的高度，去寻找可能出现的词汇结构，再分配不同的权重，以此得到这个文本是不是这个类别的文本。【上下文相似的词其语义也相似———这是出自一篇经典论文】</p><h3 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h3><p>这一层是用来挑选出各种filter的最明显的特征，一般都是用max_feature来挑选，是吧是吧，简单粗暴。然后如果还想继续加层的话，就可以在池化层后，接上卷积层，再接上池化层，以此循环迭代。</p><h3 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h3><p>这一层可以看成分类器，它的公式是：$y=w*z+b$，其中$z$当然是所有filter中的所有的最大特征值咯。最后这个$y$就被代入到激活函数里头$a = f(y)$。得出的值就是对应的属于不同类别的概率值，这些概率值就是全连接层的输出结果。</p><p>在计算出了概率值之后，根据预期结果（$\hat{y}$）和实际结果$a$计算出loss值（一般都是用$E = \frac{1}{2}\sum_{i}^{n}(\hat{y}-a)^{2}$这个公式计算出偏移值）这样就可以通过反向传播，去调整权重值和偏差值。</p><h3 id="反向传参"><a href="#反向传参" class="headerlink" title="反向传参"></a>反向传参</h3><p>还记得上面论文中说过，CNN在句子分类中，使用的是两个chennal，一个是静态chennal，一个是动态chennal，动态的 chennal是用来从全连接层反向传播调整参数的，静态的chennal是用来向传参，计算分类概率的。</p><p>反向传参的方法就是求导，从全连接层往输入层去反向推导调整权重值和偏差值，之前一直没能理解为什么反向求导时候怎么调整权重值，现在在这里把我目前的理解，讲一遍，然后如果理解错了，大家能够指正一下：</p><p>$w_{后} = w_{初} - \triangledown w$</p><p>$\triangledown w = \eta \ast \frac{\partial E}{\partial a}\ast \frac{\partial a}{\partial y}\ast \frac{\partial y}{\partial w}$</p><p>【这里只是意思意思的写那么几个公式，就是求偏导数。</p><p>当 $\triangledown w$ 为0的时候，就意味着$w$的值已经被调整到预期（最佳）值了，就不会继续调整下去了，而为什么调整$w_{后}$要用 $w_{前}$ 去减去偏导值呢？因为偏导后，得到当下的斜率值，如果是正数，说明$w_{后}$的值需要减掉 $\triangledown w $ 值得到，如果是负数，就说明$w_{后}$的值需要加上 $\triangledown w $ 值得到。除了有这个原因，<strong>如果预测值与实际值的误差越大，那么在反向传播训练的过程中，各种参数调整的幅度就要更大，从而使训练更快收敛</strong>。<a href="https://blog.csdn.net/u014313009/article/details/51043064" target="_blank" rel="noopener">来自博客</a>    </p><p>而里面的$\eta$是代表学习率，这个是自己设置的，是用来控制梯度下降的步长（这里可以看看吴恩达老师视频的梯度下降的这块），学习率太大，步长太长，就会很容易跳过让$w$能够等于最佳值的点，学习率太小，步长太小，就会下降速度太慢emmm，可以联想一下，数据不够，下降太慢==】</p><p>而 $\triangledown w$ 除了有判断梯度应该下降上升的作用以外，还有个作用就是为参数调整幅度做参考（对，可以这样说）</p><h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p>分别有relu、tanh、sigmoi，这三种函数，一般，tanh、sigmoi会被用在全连接层，relu被用在卷积层。</p><p>激活函数的作用是把卷积后的结果压缩到某一个固定的范围，这样可以一直保持一层一层下去的数值范围是可控的。———-<a href="https://www.jianshu.com/p/58168fec534d" target="_blank" rel="noopener">摘自博客</a></p><h3 id="实验参数"><a href="#实验参数" class="headerlink" title="实验参数"></a>实验参数</h3><p>$l_{2}$范式 = 3（不过我做实验用的是drop=0.5）</p><p>filter window 的h分别有：3、4、5（我用的是2、3、4）</p><p>每个高有100个feather map（60个feather map）</p><p>batch= 50（发现验证集的loss值开始下降，就停止了，设置batch值是50）</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>按照论文里面的情况来讲，全连接后进行反向传播的详细步骤是：</p><ol><li>进行卷积，池化，通过激活函数得到预测值</li><li>将预测值和期望值放在一起求出误差值</li><li>反向传播时候，将误差值代入公式求偏导运算，调整参数。</li></ol><p>这就是一篇实验报告，CNN通过不同长度的filter去搜索句子中的特征，这样的话就避免了特征遗漏的现象，然后明显化重要特征，再通过全连接层分类。这样的话有个缺点就是无法联系上下文去判断词语在文中的意义，就是无法语义理解。因为中文一词多义，所以这样分类肯定会有误差的。所以nlp比图像处理难太多了。</p><p>ending</p><hr><p>以下是我根据这个论文尝试用<code>TensorFlow</code>实现的中文文本分类的笔记。</p><p><a href="https://aimasa.github.io/2019/12/25/TensorFlow%E5%AE%9E%E7%8E%B0cnn%E8%B8%A9%E5%9D%91%E7%82%B9/#more">实验踩坑记</a></p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><a href="https://zhuanlan.zhihu.com/p/32819991" target="_blank" rel="noopener">反向传播</a></p><p><a href="https://www.aclweb.org/anthology/D14-1181.pdf" target="_blank" rel="noopener">CNN for sentence classification</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;阅读&lt;code&gt;CNN for sentence classification&lt;/code&gt;的笔记，记录一下。&lt;/p&gt;
    
    </summary>
    
      <category term="nlp自然语言处理" scheme="http://aimasa.github.io/categories/nlp%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="CNN语言模型" scheme="http://aimasa.github.io/categories/nlp%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/CNN%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"/>
    
    
      <category term="nlp" scheme="http://aimasa.github.io/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>统计学习（一）之监督学习</title>
    <link href="http://aimasa.github.io/2019/11/20/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>http://aimasa.github.io/2019/11/20/统计学习（一）/</id>
    <published>2019-11-20T03:23:56.000Z</published>
    <updated>2019-12-27T13:11:32.005Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="统计学习" scheme="http://aimasa.github.io/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="监督学习" scheme="http://aimasa.github.io/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="统计学习" scheme="http://aimasa.github.io/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>k个一组翻转链表</title>
    <link href="http://aimasa.github.io/2019/10/16/k%E4%B8%AA%E4%B8%80%E7%BB%84%E7%BF%BB%E8%BD%AC%E9%93%BE%E8%A1%A8/"/>
    <id>http://aimasa.github.io/2019/10/16/k个一组翻转链表/</id>
    <published>2019-10-16T14:11:20.000Z</published>
    <updated>2019-12-27T13:11:30.951Z</updated>
    
    <content type="html"><![CDATA[<h1 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h1><p>给你一个链表，每 k 个节点一组进行翻转，请你返回翻转后的链表。</p><p>k 是一个正整数，它的值小于或等于链表的长度。</p><p>如果节点总数不是 k 的整数倍，那么请将最后剩余的节点保持原有顺序。</p><p>示例 :</p><p><code>给定这个链表：1-&gt;2-&gt;3-&gt;4-&gt;5</code></p><p><code>当 k = 2 时，应当返回: 2-&gt;1-&gt;4-&gt;3-&gt;5</code></p><p><code>当 k = 3 时，应当返回: 3-&gt;2-&gt;1-&gt;4-&gt;5</code></p><p>说明 :</p><p>你的算法只能使用常数的额外空间。<br>你不能只是单纯的改变节点内部的值，而是需要实际的进行节点交换。</p><a id="more"></a><h1 id="心得"><a href="#心得" class="headerlink" title="心得"></a>心得</h1><p>写这道题目可以用递归，但对递归不太了解，所以明天打算写一下递归的专题，暂时不在这道题上耗时间了，等递归用的顺手的时候再回来写它。</p><h2 id="学到的点："><a href="#学到的点：" class="headerlink" title="学到的点："></a>学到的点：</h2><p>1、计算时间复杂度，虽然我在一个大循环里面包了两个不同的小循环，但是时间复杂度加在一起，一共是$f(3l-\frac{l}{k})$,并不是我想的会复杂度很高。</p><p>2、要理清楚<code>ListNode</code>的存储结构，不然容易出现指向地址的值产生循环的情况。</p><h1 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h1><p>注：里面<code>pre</code>和<code>corr</code>默认使用<code>0-&gt;xxxxxx</code>如此，虽然画的是直接指向需要转换的链表的头指针，但是他们默认是<code>.next</code>指向。</p><p>我没太看仔细看网上的思路，跟着自己的想法写了这个，用语言不知道要怎么表达，干脆放图好了：</p><p><center><img src="http://pictures.aimasa.club/static/images/k个一组翻转链表\example.png"></center><br>在上图中，先考虑只改变<code>ListNode</code>链表首的翻转变动。$k = 1$的时候链表不做改动，$k = 2$的时候两两交换，$k = 3$的时候就把$k=2$时候翻转好的链表作为一个整体，再和指向的第三个链表做一次两两交换，$k=4$就可以把$k=3$的已经转换好的链表看做一个整体，再与指向的第四个链表做一次两两交换，以此类推。</p><p>这样就可以发现可以根据$k$数量的做一次循环了，这个循环的复杂度只有$f(k)$，需要的指针如下所示：</p><p><center><img src="http://pictures.aimasa.club/static/images/k个一组翻转链表\zhizhen.png"></center><br>指针<code>end</code>永远都是跟在<code>second</code>的后面，指向链表尾部，<code>second</code>指向<code>end</code>的前一个的原因就是上面说的将前一部分看做一个整体，和后面指向的数字进行两两交换，<code>first-&gt;second</code>就是那个整体，<code>end</code>是那个后面指向的数字。所以<code>first</code>永远都是指在要交换的链表头（就是比如图上<code>1-&gt;2-&gt;3-&gt;4</code>时候，$k=3$，第一次要对<code>1-&gt;2-&gt;3</code>交换，那<code>first</code>指向链表中存储<code>1</code>的位置，当前面三个交换完之后，对下一组进行交换，那么<code>first</code>就应该指向<code>4</code>，因为它是下一组的链表之首。）</p><p><code>pre.next</code>这个指针我用来存储头节点，到最后可以通过输出它，而输出已经被翻转好的链表。</p><p>但是，因为<code>end</code>指针会一直和<code>first</code>指针进行交换，所以<code>pre.next</code>指针指向的地址只会一直跟着<code>first</code>，而无法将转换好的链表完整输出：</p><p><center><img src="http://pictures.aimasa.club/static/images/k个一组翻转链表\preerror.png"></center><br>所以我加了一个<code>corr</code>指针去跟踪这个翻转后的完整的链表，让<code>corr</code>指向翻转好的链表地址。</p><p><center><img src="http://pictures.aimasa.club/static/images/k个一组翻转链表\process.png"></center><br>这样，就得到正确的结果了。</p><p>为了知道有多少段链表需要我去进行翻转，而不用临时判空等操作浪费时间，所以在用循环知道了链表会有多长之后，整除$k$，知道了有多少段链表需要翻转，然后再整合上面的思路。</p><h1 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h1><p>emm我觉得递归的话可能效率更高？因为时间都浪费在让<code>occr</code>这个指针去跟踪更新的链表了。</p><h1 id="题目来源"><a href="#题目来源" class="headerlink" title="题目来源"></a>题目来源</h1><p><a href="https://leetcode-cn.com/problems/reverse-nodes-in-k-group/" target="_blank" rel="noopener">leedcode</a></p><h1 id="参考代码"><a href="#参考代码" class="headerlink" title="参考代码"></a>参考代码</h1><p><a href="https://github.com/aimasa/exercise_demo/tree/master/src/exercise/demo/mergeklists" target="_blank" rel="noopener">参考代码</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h1&gt;&lt;p&gt;给你一个链表，每 k 个节点一组进行翻转，请你返回翻转后的链表。&lt;/p&gt;
&lt;p&gt;k 是一个正整数，它的值小于或等于链表的长度。&lt;/p&gt;
&lt;p&gt;如果节点总数不是 k 的整数倍，那么请将最后剩余的节点保持原有顺序。&lt;/p&gt;
&lt;p&gt;示例 :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;给定这个链表：1-&amp;gt;2-&amp;gt;3-&amp;gt;4-&amp;gt;5&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;当 k = 2 时，应当返回: 2-&amp;gt;1-&amp;gt;4-&amp;gt;3-&amp;gt;5&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;当 k = 3 时，应当返回: 3-&amp;gt;2-&amp;gt;1-&amp;gt;4-&amp;gt;5&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;说明 :&lt;/p&gt;
&lt;p&gt;你的算法只能使用常数的额外空间。&lt;br&gt;你不能只是单纯的改变节点内部的值，而是需要实际的进行节点交换。&lt;/p&gt;
    
    </summary>
    
      <category term="算法" scheme="http://aimasa.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
      <category term="k个一组翻转链表" scheme="http://aimasa.github.io/categories/%E7%AE%97%E6%B3%95/k%E4%B8%AA%E4%B8%80%E7%BB%84%E7%BF%BB%E8%BD%AC%E9%93%BE%E8%A1%A8/"/>
    
    
      <category term="算法" scheme="http://aimasa.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="leedcode算法题" scheme="http://aimasa.github.io/tags/leedcode%E7%AE%97%E6%B3%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>爬虫心得</title>
    <link href="http://aimasa.github.io/2019/10/15/%E7%88%AC%E8%99%AB%E5%BF%83%E5%BE%97/"/>
    <id>http://aimasa.github.io/2019/10/15/爬虫心得/</id>
    <published>2019-10-15T06:04:43.000Z</published>
    <updated>2019-12-27T13:11:31.958Z</updated>
    
    <content type="html"><![CDATA[<p>因为毕业设计的缘故，网上没有现成的数据库，所以要自己想办法去爬取数据。所以</p><p>毕业设计step1:</p><ol><li>书写爬虫爬取文本</li></ol><p>之前试着写了一个爬漫画的爬虫练手（书写思路另会写一条博客详细说明），那时候是对页面漫画的内容进行爬取，但是这次写的爬虫是对页面<code>html</code>的信息进行爬取。</p><a id="more"></a><p>拟使用的库是<code>requests_html</code>和<code>PyQuery</code>，但是<code>PyQuery</code>这个单独的库好像就能满足我的需求。</p><p>其中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery <span class="keyword">as</span> pq</span><br></pre></td></tr></table></figure><p>从<code>PyQuery</code>导入<code>pyquery</code>的方法模块。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pq(link)</span><br></pre></td></tr></table></figure><p>这句话的意思是获取这个<code>link</code>对应的页面的<code>html</code>文件。但是我这样写的时候却出现了乱码。</p><p>要根据网站的编码方式再去进行解码，所以我直接去获取这个网址对应的<code>html</code>信息是会返回乱码的。所以要把这句话改成：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pq(link,encoding=<span class="string">""</span>)</span><br></pre></td></tr></table></figure><p>现在我们就需要知道网站编码格式是什么了，我们打开<code>link</code>对应的那个网站页面，点击<code>f12</code>看它的页面元素（位置一般在<code>&lt;head&gt;</code>这个标签里面）：</p><center><img src="http://pictures.aimasa.club/static/images/爬虫心得\content.png"></center><p>由此可知，<code>encoding=&quot;gb2312&quot;</code></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;因为毕业设计的缘故，网上没有现成的数据库，所以要自己想办法去爬取数据。所以&lt;/p&gt;
&lt;p&gt;毕业设计step1:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;书写爬虫爬取文本&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;之前试着写了一个爬漫画的爬虫练手（书写思路另会写一条博客详细说明），那时候是对页面漫画的内容进行爬取，但是这次写的爬虫是对页面&lt;code&gt;html&lt;/code&gt;的信息进行爬取。&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://aimasa.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
      <category term="爬取html信息" scheme="http://aimasa.github.io/categories/%E7%88%AC%E8%99%AB/%E7%88%AC%E5%8F%96html%E4%BF%A1%E6%81%AF/"/>
    
    
      <category term="爬虫" scheme="http://aimasa.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>两两交换链表中的节点</title>
    <link href="http://aimasa.github.io/2019/10/14/%E4%B8%A4%E4%B8%A4%E4%BA%A4%E6%8D%A2%E9%93%BE%E8%A1%A8%E4%B8%AD%E7%9A%84%E8%8A%82%E7%82%B9/"/>
    <id>http://aimasa.github.io/2019/10/14/两两交换链表中的节点/</id>
    <published>2019-10-14T13:00:45.000Z</published>
    <updated>2019-12-27T13:11:31.028Z</updated>
    
    <content type="html"><![CDATA[<h1 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h1><p>给定一个链表，两两交换其中相邻的节点，并返回交换后的链表。</p><p>你不能只是单纯的改变节点内部的值，而是需要实际的进行节点交换。</p><center><img src="http://pictures.aimasa.club/static/images/两两交换链表中的节点\buzou.png"></center><p>示例:</p><p>给定 1-&gt;2-&gt;3-&gt;4, 你应该返回 2-&gt;1-&gt;4-&gt;3.</p><a id="more"></a><p>这题的话用之前的思路也是行得通的，就是两两交换的思路可以通过<code>while</code>进行循环，然后两两节点换位置，类似于普通参数换位置，因为它可以分解成一个个重复的小问题，所以这题还能用递归去解决。</p><p>因为我递归总是不太熟，所以在这里特地写了个笔记。</p><h1 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h1><p>把这个题目的大问题划分成一个个重复的子问题（对链表头进行节点互换），然后再限定条件，找到出口点（节点不能为空，节点的<code>.next</code>不能为空）</p><p>然后根据这个思路把代码表示出来。</p><p>在<a href="https://leetcode-cn.com/problems/swap-nodes-in-pairs" target="_blank" rel="noopener">leedcode</a>网站中，标准题解中对递归解释是这样的：递归写法要观察本级递归的解决过程，形成抽象模型，因为递归本质就是不断重复相同的事情。而不是去思考完整的调用栈，一级又一级，无从下手。如图所示，我们应该关注一级调用小单元的情况，也就是单个f(x)。</p><h2 id="参考网站"><a href="#参考网站" class="headerlink" title="参考网站"></a>参考网站</h2><p><a href="https://leetcode-cn.com/problems/swap-nodes-in-pairs" target="_blank" rel="noopener">力扣</a></p><h2 id="参考代码"><a href="#参考代码" class="headerlink" title="参考代码"></a>参考代码</h2><p><a href="https://github.com/aimasa/exercise_demo/tree/master/src/exercise/demo/swappairs" target="_blank" rel="noopener">代码</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h1&gt;&lt;p&gt;给定一个链表，两两交换其中相邻的节点，并返回交换后的链表。&lt;/p&gt;
&lt;p&gt;你不能只是单纯的改变节点内部的值，而是需要实际的进行节点交换。&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;http://pictures.aimasa.club/static/images/两两交换链表中的节点\buzou.png&quot;&gt;&lt;/center&gt;

&lt;p&gt;示例:&lt;/p&gt;
&lt;p&gt;给定 1-&amp;gt;2-&amp;gt;3-&amp;gt;4, 你应该返回 2-&amp;gt;1-&amp;gt;4-&amp;gt;3.&lt;/p&gt;
    
    </summary>
    
      <category term="算法" scheme="http://aimasa.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
      <category term="两两交换链表中的节点" scheme="http://aimasa.github.io/categories/%E7%AE%97%E6%B3%95/%E4%B8%A4%E4%B8%A4%E4%BA%A4%E6%8D%A2%E9%93%BE%E8%A1%A8%E4%B8%AD%E7%9A%84%E8%8A%82%E7%82%B9/"/>
    
    
      <category term="算法" scheme="http://aimasa.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="leedcode算法题" scheme="http://aimasa.github.io/tags/leedcode%E7%AE%97%E6%B3%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>Http协议</title>
    <link href="http://aimasa.github.io/2019/09/25/Http%E5%8D%8F%E8%AE%AE/"/>
    <id>http://aimasa.github.io/2019/09/25/Http协议/</id>
    <published>2019-09-25T01:38:23.000Z</published>
    <updated>2019-12-27T13:11:30.603Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Lezhin爬虫使用</title>
    <link href="http://aimasa.github.io/2019/09/08/lezhincomic/"/>
    <id>http://aimasa.github.io/2019/09/08/lezhincomic/</id>
    <published>2019-09-08T10:19:11.000Z</published>
    <updated>2019-09-20T02:45:08.872Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Lezhin爬虫使用方法"><a href="#Lezhin爬虫使用方法" class="headerlink" title="Lezhin爬虫使用方法"></a>Lezhin爬虫使用方法</h1><p>lezhin爬虫配置文件介绍</p><a id="more"></a><h2 id="配置文件模板"><a href="#配置文件模板" class="headerlink" title="配置文件模板"></a>配置文件模板</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#需要爬的漫画信息</span><br><span class="line">[comic_info]</span><br><span class="line">comic_chinese_name = 我的哥哥我的老师</span><br><span class="line">comic_name = mybromyssam</span><br><span class="line">series_id_first = 25</span><br><span class="line">series_id_last = 25</span><br><span class="line">; ---------------------------------</span><br><span class="line">; 需要存放的文件路径以及压缩格式</span><br><span class="line">[folder_info]</span><br><span class="line">zip_type = zip</span><br><span class="line">folder_name_header = H:/</span><br><span class="line"></span><br><span class="line">; ----------------------------------</span><br><span class="line">; 账号登录网站后得到的必要信息</span><br><span class="line">[comic_request_info]</span><br><span class="line">access_token = </span><br><span class="line">comic_id = </span><br><span class="line">lezhin_cookie = </span><br><span class="line"></span><br><span class="line">;------------------------------------</span><br><span class="line">;下载完一话漫画下第二话需要等待时间</span><br><span class="line">[spare_time]</span><br><span class="line">spare_time = 10</span><br></pre></td></tr></table></figure><p>注：配置文件后缀名为 .ini，也可以是.config</p><p><strong>小白使用方法</strong></p><ul><li>该脚本打包成的exe使用环境为 window10（我觉得window系列的系统都能用它，我没试过，因为我的电脑是window10，所以我这里标注的使用环境仅为window10）</li><li>新建配置文件在自己电脑里（文件名格式：xxx.ini 或 xxx.config 。ps：没试过用中文的文件名，你们可以试试。）</li><li>运行之后，输入配置文件地址</li><li>中断运行的话按<code>ctrl+c</code></li></ul><p><strong>开发者使用方法</strong></p><ul><li>下载源码，里面缺失的python包都能通过<code>pip install xxx</code>下载得到。</li><li>新建配置文件在自己电脑里（文件名格式：xxx.ini 或 xxx.config 。ps：没试过用中文的文件名，你们可以试试。）</li><li>入口地址为main.py。当然run.py里面也有入口，emmm我自己写着调试的，可以不用管它。</li><li>运行后会要求输入配置文件地址，按配置文件在自己电脑里存放的位置输入即可。</li></ul><p>只能爬取自己已经购买了的漫画，或者网站确定为免费的漫画。</p><p>打包后的脚本并未开放。</p><h2 id="配置参数详解："><a href="#配置参数详解：" class="headerlink" title="配置参数详解："></a>配置参数详解：</h2><h3 id="comic-name、comic-id"><a href="#comic-name、comic-id" class="headerlink" title="comic_name、comic_id"></a>comic_name、comic_id</h3><p><center><img src="http://pictures.aimasa.club/static/images/GitReadMePicture\LezhinComic\first.png"></center><br>点进去之后就能在浏览器的路径上找到对应漫画的comic_name</p><p><center><img src="http://pictures.aimasa.club/static/images/GitReadMePicture\LezhinComic\getcomicname.png"></center><br>接着往下拉，按下F12点，进想要下载的漫画的第一话（随便哪话都能找到以下信息）</p><p>=========================================================================================================</p><p>重点分割线</p><p>======================================================================================================</p><p>点开<code>NetWork</code>这栏</p><p><center><img src="http://pictures.aimasa.club/static/images/GitReadMePicture\LezhinComic\getLezhincookie.jpg"></center><br>找到红框中的链接内容，点开之后会在旁边弹出一个小框</p><p><center><img src="http://pictures.aimasa.club/static/images/GitReadMePicture\LezhinComic\comic_nameandid.jpg"></center><br>在这个小框中，划拉到<code>header</code>这栏最下面，会看到Query xxxx，里面alias就是配置文件里对应的<code>comic_name</code>，<code>_</code>的值就是配置文件里的<code>comic_id</code>【ps:<code>comic_id</code>是我命名有问题，我也不知道<code>_</code>是代表什么。】</p><h3 id="lezhin-cookie"><a href="#lezhin-cookie" class="headerlink" title="lezhin_cookie"></a>lezhin_cookie</h3><p>在<code>header</code>这栏中间会看到<code>cookie</code>，这个值是<code>lezhin_cookie</code></p><p><center><img src="http://pictures.aimasa.club/static/images/GitReadMePicture\LezhinComic\cookie.jpg"></center></p><p><center><img src="http://pictures.aimasa.club/static/images/GitReadMePicture\LezhinComic\togetaccesstoken.jpg"></center></p><h3 id="access-token"><a href="#access-token" class="headerlink" title="access_token"></a>access_token</h3><p>按<code>F5</code>刷新，切换按钮到红框指定的位置，根据下图的指示，能够找到配置文件中<code>access_token</code>对应的值。</p><p><center><img src="http://pictures.aimasa.club/static/images/GitReadMePicture\LezhinComic\getaccesstoken.jpg"></center><br>【因为如果哪天爬不到文件，有一个原因是cookie失效，或者access_token失效，那时候就需要重复一遍获取cookie和access_token的步骤去更新它们】</p><h3 id="comic-chinese-name、series-id-first、series-id-last、zip-type、folder-name-header、spare-time"><a href="#comic-chinese-name、series-id-first、series-id-last、zip-type、folder-name-header、spare-time" class="headerlink" title="comic_chinese_name、series_id_first、series_id_last、zip_type、folder_name_header、spare_time"></a>comic_chinese_name、series_id_first、series_id_last、zip_type、folder_name_header、spare_time</h3><p>这些参数都是自己可以随便定义的。</p><p><code>comic_chinese_name</code>：自己定义的漫画的中文名</p><p><code>series_id_first、series_id_last</code>：想要下载漫画的集数范围。例：如果想要下载该漫画1-15集，那么<code>series_id_first=1</code>，<code>series_id_last=15</code>  ;如果只想下载15集，那么<code>series_id_first = 15</code>，<code>series_id_last=15</code></p><p><code>zip_type</code>：想压缩成的格式。例：<code>zip_type = zip</code>，文件则被压缩成zip格式</p><p><code>folder_name_header</code>：想要存储的地址前缀。例：想要把<code>我的哥哥我的老师</code>漫画全部存在<code>D:/comic</code>里面，然后<code>folder_name_header = D:/comic</code>，运行此脚本，会在<code>D:/comic</code>里面生成一个<code>我的哥哥我的老师</code>这个总文件夹，里面会有你下载的漫画集数（下载下来的图片是分集装好）。</p><p><code>spare_time</code>：爬完上一话，继续爬下一话时候的间隔时间。因为怕爬取漫画过快，被检测发现是用了脚本在爬，所以加了一个<code>spare_time</code>，可以<code>spare_time=0</code>，但是可能会有被封号的风险哟。<code>spare_time=1</code>，就意思是间隔时间为1s。</p><blockquote><p>eg:    alias : myxxxx  =&gt;  comic_name = myxxxx</p></blockquote><p>接着把自己的配置文件在本地的存放位置在程序开始时填一下，如：配置文件名为：<code>lezhin.config</code>，放在电脑的<code>D:/configfile</code>这个文件夹下面，运行程序，出现<code>配置文件路径：</code>的字样时候，输入<code>D:/配置文件/lezhin.config</code>。好了，回车，开始爬数据了，如果突然闪退，可以看log里面的记录。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这个脚本对我来说现阶段最麻烦的就是cookie和access_token这两个参数需要手动获取。emmm看看能不能改进吧。</p><p>因为是在闲暇时候摸鱼写出来的脚本，可能会有很多不足，欢迎大家给我提issue。</p><h2 id="爬虫代码"><a href="#爬虫代码" class="headerlink" title="爬虫代码"></a>爬虫代码</h2><p><a href="https://github.com/aimasa/LezhinComic" target="_blank" rel="noopener">lezhin爬虫</a></p><h2 id="注：禁止用此爬虫爬下来的漫画进行二次贩卖！！！！！！！！！！请尊重他人版权！！！！！！！！！！！！！！！！此爬虫仅做学习用途！！！！！！！！！！！！！"><a href="#注：禁止用此爬虫爬下来的漫画进行二次贩卖！！！！！！！！！！请尊重他人版权！！！！！！！！！！！！！！！！此爬虫仅做学习用途！！！！！！！！！！！！！" class="headerlink" title="注：禁止用此爬虫爬下来的漫画进行二次贩卖！！！！！！！！！！请尊重他人版权！！！！！！！！！！！！！！！！此爬虫仅做学习用途！！！！！！！！！！！！！"></a>注：禁止用此爬虫爬下来的漫画进行二次贩卖！！！！！！！！！！请尊重他人版权！！！！！！！！！！！！！！！！此爬虫仅做学习用途！！！！！！！！！！！！！</h2><p>注：禁止用此爬虫爬下来的漫画进行二次贩卖！！！！！！！！！！请尊重他人版权！！！！！！！！！！！！！！！！此爬虫仅做学习用途！！！！！！！！！！！！！</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Lezhin爬虫使用方法&quot;&gt;&lt;a href=&quot;#Lezhin爬虫使用方法&quot; class=&quot;headerlink&quot; title=&quot;Lezhin爬虫使用方法&quot;&gt;&lt;/a&gt;Lezhin爬虫使用方法&lt;/h1&gt;&lt;p&gt;lezhin爬虫配置文件介绍&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://aimasa.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
      <category term="漫画" scheme="http://aimasa.github.io/categories/%E7%88%AC%E8%99%AB/%E6%BC%AB%E7%94%BB/"/>
    
    
      <category term="爬虫" scheme="http://aimasa.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>python语法记录</title>
    <link href="http://aimasa.github.io/2019/08/21/python%E8%AF%AD%E6%B3%95%E8%AE%B0%E5%BD%95/"/>
    <id>http://aimasa.github.io/2019/08/21/python语法记录/</id>
    <published>2019-08-21T03:17:19.000Z</published>
    <updated>2019-11-08T03:04:37.870Z</updated>
    
    <content type="html"><![CDATA[<p>这是在学习python时候碰到的一些语法问题，先记下来，方便日后使用</p><a id="more"></a><h2 id="numpy相关记录"><a href="#numpy相关记录" class="headerlink" title="numpy相关记录"></a>numpy相关记录</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">power(vector2 - vector1, <span class="number">2</span>)</span><br></pre></td></tr></table></figure><p><code>power</code>是<code>numpy</code>这个类的一个运算函数，是用来计算次方的，上面那行代码意思是计算$(vector2 - vector1)^{2}$,所以<code>power(x,y)</code>是计算$\,x^{y}\,$这个公式的.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sum(power(vector2 - vector1, <span class="number">2</span>))</span><br></pre></td></tr></table></figure><p><code>sum</code>也是<code>numpy</code>这个类的一个运算函数，但是在上式中vector是一个二维数组（可以这样说，是n行两列的矩阵中取的第一行），<code>num(x,y)</code>运算是$x + y$，所以里面就是<code>power(vector2 - vector1, 2)</code>计算出来的数组里面的第一行第一列和第二行第二列的数值相加。（等有机会试试<code>sum</code>的括号里放多行两列数组）</p><h2 id="pandas相关记录"><a href="#pandas相关记录" class="headerlink" title="pandas相关记录"></a>pandas相关记录</h2><p><code>read_csv()</code>：默认返回值是<code>DataFrame</code></p><p>而加上了<code>chunksize=xxx</code>这个参数之后返回值就变成了：<code>TextFileReader</code></p><p>在试运行文本处理的程序时候需要进行数据预处理，作者说是先在本机几条数据看看效果，所以我通过加参数<code>chunksize</code>把数据分块成小块这样，注意：并不是单纯的分割出来前多少条数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">test_data = pd.DataFrame(pd.read_csv(<span class="string">'G:/Data using/new_data/new_data/test_set.csv'</span>,chunksize= <span class="number">1000</span>).get_chunk(<span class="number">1</span>))</span><br></pre></td></tr></table></figure><p>这是取分块成功后的第一块（就是前一千条哦）。是从1开始计数的。</p><p><code>G:/Data using/new_data/new_data/test_set.csv</code>这是我存放数据的地址。</p><p>​        </p><p>除掉这种<code>pd.DataFrame().get_chunk(1)</code>的办法还有拼接的办法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">test_data = pd.concat(pd.read_csv(<span class="string">'G:/Data using/new_data/new_data/test_set.csv'</span>,chunksize= <span class="number">1000</span>), ignore_index=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><h2 id="requests-html"><a href="#requests-html" class="headerlink" title="requests_html"></a>requests_html</h2><h1 id="collection相关记录"><a href="#collection相关记录" class="headerlink" title="collection相关记录"></a>collection相关记录</h1><p>collection.OrderedDict()是按放入元素的先后顺序进行排序。（有序字典）</p><h1 id="普通语法"><a href="#普通语法" class="headerlink" title="普通语法"></a>普通语法</h1><p>cls()：让我感觉像是封装，就是把一些参数经过处理然后封装起来。</p><p><code>unicodedata.normalize()</code> 第一个参数指定字符串标准化的方式。 NFC表示字符应该是整体组成(比如可能的话就使用单一编码)，而NFD表示字符应该分解为多个组合字符表示。</p><ul><li>unicodedata.category(chr)  </li></ul><p>把一个字符返回它在UNICODE里分类的类型。具体类型如下：</p><p>Code    Description</p><p>[Cc]    Other, Control</p><p>[Cf]    Other, Format</p><p>[Cn]    Other, Not Assigned (no characters in the file have this property)</p><p>[Co]    Other, Private Use</p><p>[Cs]    Other, Surrogate</p><p>[LC]    Letter, Cased</p><p>[Ll]    Letter, Lowercase</p><p>[Lm]    Letter, Modifier</p><p>[Lo]    Letter, Other</p><p>[Lt]    Letter, Titlecase</p><p>[Lu]    Letter, Uppercase</p><p>[Mc]    Mark, Spacing Combining</p><p>[Me]    Mark, Enclosing</p><p>[Mn]    Mark, Nonspacing</p><p>[Nd]    Number, Decimal Digit</p><p>[Nl]    Number, Letter</p><p>[No]    Number, Other</p><p>[Pc]    Punctuation, Connector</p><p>[Pd]    Punctuation, Dash</p><p>[Pe]    Punctuation, Close</p><p>[Pf]    Punctuation, Final quote (may behave like Ps or Pe depending on usage)</p><p>[Pi]    Punctuation, Initial quote (may behave like Ps or Pe depending on usage)</p><p>[Po]    Punctuation, Other</p><p>[Ps]    Punctuation, Open</p><p>[Sc]    Symbol, Currency</p><p>[Sk]    Symbol, Modifier</p><p>[Sm]    Symbol, Math</p><p>[So]    Symbol, Other</p><p>[Zl]    Separator, Line</p><p>[Zp]    Separator, Paragraph</p><p>[Zs]    Separator, Space</p><p>使用一个类，然后用<code>__init__</code>去初始化它，再返回值，相当于返回了一个封装好了的对象</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">state_dict = kwargs.get(<span class="string">'state_dict'</span>, <span class="keyword">None</span>)</span><br></pre></td></tr></table></figure><p>这是<em>args和*</em>kwargs的使用方法。如果未给出该值则state_dict值为None</p><p><a href="https://stackoverflow.com/questions/36901/what-does-double-star-asterisk-and-star-asterisk-do-for-parameters" target="_blank" rel="noopener">参考</a></p><p>sys.version_info[0]可以得到python的版本</p><p>Python getattr() 函数   </p><p>pow(n) 是计算n次方的</p><h2 id="参考博客"><a href="#参考博客" class="headerlink" title="参考博客"></a>参考博客</h2><p><a href="https://stackoverflow.com/questions/39386458/how-to-read-data-in-python-dataframe-without-concatenating" target="_blank" rel="noopener">pandas TextFileReader  to DataFrame</a></p><p><a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html" target="_blank" rel="noopener">pandas官方文档</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这是在学习python时候碰到的一些语法问题，先记下来，方便日后使用&lt;/p&gt;
    
    </summary>
    
      <category term="python语法" scheme="http://aimasa.github.io/categories/python%E8%AF%AD%E6%B3%95/"/>
    
      <category term="各大库的琐碎语法记录" scheme="http://aimasa.github.io/categories/python%E8%AF%AD%E6%B3%95/%E5%90%84%E5%A4%A7%E5%BA%93%E7%9A%84%E7%90%90%E7%A2%8E%E8%AF%AD%E6%B3%95%E8%AE%B0%E5%BD%95/"/>
    
    
      <category term="python语法" scheme="http://aimasa.github.io/tags/python%E8%AF%AD%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>toString()和new String()</title>
    <link href="http://aimasa.github.io/2019/08/19/java%E7%9A%84%E8%BD%AC%E6%8D%A2String/"/>
    <id>http://aimasa.github.io/2019/08/19/java的转换String/</id>
    <published>2019-08-19T02:38:25.000Z</published>
    <updated>2019-09-09T06:34:34.909Z</updated>
    
    <content type="html"><![CDATA[<p>用byte[]转换成String类型的时候，我用了(byte[]).toString()去转换，然后返回的结果居然是xxx@xxxx这个字符串，然后我很懵，点进源码才明白原因。</p><a id="more"></a><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> getClass().getName() + <span class="string">"@"</span> + Integer.toHexString(hashCode());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这是把类的类名加上“@”加上做的hashcode值转换成字符串返回输出，byte[]是个数组，是一个对象，所有的对象都是object的子类，所以数组可以使用object的方法，在object的方法里有toString()这个方法，但在没有被复写的情况下使用它，就会生成<code>getClass().getName() + &quot;@&quot; + Integer.toHexString(hashCode())</code>这个字符串。</p><p>所以我们用new String()的方式去转换byte[]值，将其变成String字符串输出。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">String</span><span class="params">(<span class="keyword">byte</span> bytes[])</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>(bytes, <span class="number">0</span>, bytes.length);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">String</span><span class="params">(<span class="keyword">byte</span> bytes[], <span class="keyword">int</span> offset, <span class="keyword">int</span> length)</span> </span>&#123;</span><br><span class="line">    checkBounds(bytes, offset, length);</span><br><span class="line">    <span class="keyword">this</span>.value = StringCoding.decode(bytes, offset, length);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这是这两种方法的输出结果</p><center><img src="http://pictures.aimasa.club/static/images/java的转换String/example.png"></center><h1 id="参考博客"><a href="#参考博客" class="headerlink" title="参考博客"></a>参考博客</h1><p><a href="https://blog.csdn.net/mrbacker/article/details/81638331" target="_blank" rel="noopener">数组相关概念</a></p><p><a href="https://zhidao.baidu.com/question/267505870.html" target="_blank" rel="noopener">关于object的子类</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;用byte[]转换成String类型的时候，我用了(byte[]).toString()去转换，然后返回的结果居然是xxx@xxxx这个字符串，然后我很懵，点进源码才明白原因。&lt;/p&gt;
    
    </summary>
    
      <category term="java语法" scheme="http://aimasa.github.io/categories/java%E8%AF%AD%E6%B3%95/"/>
    
      <category term="toString()和new String()" scheme="http://aimasa.github.io/categories/java%E8%AF%AD%E6%B3%95/toString-%E5%92%8Cnew-String/"/>
    
    
      <category term="java语法" scheme="http://aimasa.github.io/tags/java%E8%AF%AD%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>电话号码的字母组合</title>
    <link href="http://aimasa.github.io/2019/08/05/%E7%94%B5%E8%AF%9D%E5%8F%B7%E7%A0%81%E7%9A%84%E5%AD%97%E6%AF%8D%E7%BB%84%E5%90%88/"/>
    <id>http://aimasa.github.io/2019/08/05/电话号码的字母组合/</id>
    <published>2019-08-05T03:09:32.000Z</published>
    <updated>2019-10-17T01:00:53.078Z</updated>
    
    <content type="html"><![CDATA[<p>给定一个仅包含数字 <code>2-9</code> 的字符串，返回所有它能表示的字母组合。</p><p>给出数字到字母的映射如下（与电话按键相同）。注意 1 不对应任何字母。</p><center><img src="http://pictures.aimasa.club/static/images/电话号码的字母组合/problem.png"></center><p><a href="https://leetcode-cn.com/problems/letter-combinations-of-a-phone-number/" target="_blank" rel="noopener">题目来源</a></p><a id="more"></a><h1 id="解答方法"><a href="#解答方法" class="headerlink" title="解答方法"></a>解答方法</h1><p>这个题目暴力破解的话至少要双重循环，更甚，然后我在这题里面学习了一下怎么用递归。</p><h2 id="关于递归"><a href="#关于递归" class="headerlink" title="关于递归"></a>关于递归</h2><p>递归的话就是把一个庞大的问题细化成一个细小的问题，当然，这个庞大的问题的必须是可以重复的，机械的问题。</p><p>我特地去找了一下递归的说明：</p><ol><li><strong>明确递归终止条件</strong>：就是让递归及时终止，这样递归就不会一直一直执行下去</li><li><strong>给出递归终止时的处理办法</strong>：终止时候需要执行什么，这也是需要写的。</li><li><strong>提取重复的逻辑，缩小问题规模</strong></li></ol><p>里面重要的还是对递归问题进行提取重复逻辑。</p><p>因为我对递归不是很了解，所以我打算在这记一下我跟着官方题解弄懂的怎么缩小大的问题规模</p><h2 id="分析逻辑"><a href="#分析逻辑" class="headerlink" title="分析逻辑"></a>分析逻辑</h2><p><center><img src="http://pictures.aimasa.club/static/images/电话号码的字母组合/tree.png"></center><br>假设输入的数字是“23“，然后”2“分支出去”abc“，三个支，再由”abc“分支，每个支都会连接到3里面的每个分支。</p><p>这就是可以被提取出来的重复逻辑。</p><p>可以从“2”中的分支递归。我们先把重复逻辑提取出来，重复的逻辑就是把数字对应的字母分支开来，然后继续分支下去，如此循环往复。</p><p>所以就用到了for循环，先对分支的顶端进行处理，也就是“2”分出来的“abc”三个分支。</p><p>我们要把结果存储到list列表里面，那么在递归结束后，要把输出的结果add进list中</p><p>流程就变成了这样：</p><p><center><img src="http://pictures.aimasa.club/static/images/电话号码的字母组合/info.png"></center><br>【注：里面有错别字：是==第一个循环==】</p><p>这样就能把这个分支用递归的办法全部遍历一遍了。</p><p>也就是：我从第一层开始就对“2”指向的全部字母进行一个循环，循环中我再对跟在“2”后面的“3”指向的字母再次进行一个循环（也就是循环中进行递归），在循环的过程中，如果“3”后面还跟有其他数字的话，那我继续对“3”后面跟着的数字指向的字母进行循环，但如果“3”是最后一个数字的话，我把“3”指向的字母遍历完就可以返回上一层递归了，直到最后返回到最顶层，返回最终的结果。</p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p><a href="https://blog.csdn.net/HD243608836/article/details/79973010" target="_blank" rel="noopener">关于递归的特点以及注意点</a></p><p><a href>代码实现</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;给定一个仅包含数字 &lt;code&gt;2-9&lt;/code&gt; 的字符串，返回所有它能表示的字母组合。&lt;/p&gt;
&lt;p&gt;给出数字到字母的映射如下（与电话按键相同）。注意 1 不对应任何字母。&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;http://pictures.aimasa.club/static/images/电话号码的字母组合/problem.png&quot;&gt;&lt;/center&gt;


&lt;p&gt;&lt;a href=&quot;https://leetcode-cn.com/problems/letter-combinations-of-a-phone-number/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;题目来源&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="算法" scheme="http://aimasa.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
      <category term="电话号码的字母组合" scheme="http://aimasa.github.io/categories/%E7%AE%97%E6%B3%95/%E7%94%B5%E8%AF%9D%E5%8F%B7%E7%A0%81%E7%9A%84%E5%AD%97%E6%AF%8D%E7%BB%84%E5%90%88/"/>
    
    
      <category term="算法" scheme="http://aimasa.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="leedcode算法题" scheme="http://aimasa.github.io/tags/leedcode%E7%AE%97%E6%B3%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>三数之和</title>
    <link href="http://aimasa.github.io/2019/07/22/%E4%B8%89%E6%95%B0%E4%B9%8B%E5%92%8C/"/>
    <id>http://aimasa.github.io/2019/07/22/三数之和/</id>
    <published>2019-07-22T06:14:29.000Z</published>
    <updated>2019-10-17T01:01:11.318Z</updated>
    
    <content type="html"><![CDATA[<p>给定一个包含 n 个整数的数组 nums，判断 nums 中是否存在三个元素 a，b，c ，使得 a + b + c = 0 ？找出所有满足条件且不重复的三元组。</p><p>注意：答案中不可以包含重复的三元组。</p><p>例如, 给定数组 <code>nums = [-1, 0, 1, 2, -1, -4]</code>，</p><p>满足要求的三元组集合为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">  [-1, 0, 1],</span><br><span class="line">  [-1, -1, 2]</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p><a href="https://leetcode-cn.com/problems/3sum" target="_blank" rel="noopener">力扣（LeetCode）</a></p><a id="more"></a><p>先排序（<code>java</code>里面有<code>Arrays.sort()</code>这个方法进行排序，不过我自己用的是自己写的二分排序对这个数组进行的排序）</p><p>排完序，就能够更方便的去查找满足条件并且不重复的三元组了。</p><p>我们对三元组进行与两元组查找相似的操作，不过三元组多了一个元素而已。</p><p>所以先介绍两元组的和的查找办法</p><h2 id="两元组相加办法"><a href="#两元组相加办法" class="headerlink" title="两元组相加办法"></a>两元组相加办法</h2><p><center><img src="http://pictures.aimasa.club/static/images/三数之和\first.png"></center><br>因为现在这个数组是经过排序预处理的数组，所以接下来的查找中，left指向的数字和right指向的数字相加，比预计和要大的话，那么left就要向左边移动一位，就是<code>left--</code>；如果比预计和小的话，那么right要往右边移动一位，就是<code>right++</code>。</p><p>因为题目中要求的二元组是不能有重复的元素存在，所以在指针移动的过程中，我们要去对指针指向的数据进行判断，如果<code>right</code>和<code>left</code>指向的下一个元素与自己指向的上一个元素相同的时候，可以直接跳过，无需再次进行计算。（两点确定一条直线）</p><p><a href="https://github.com/aimasa/exercise_demo/tree/master/src/exercise/demo/twosun" target="_blank" rel="noopener">代码示例</a></p><h2 id="三元组相加办法"><a href="#三元组相加办法" class="headerlink" title="三元组相加办法"></a>三元组相加办法</h2><p>三元组比两元组多了一个元素，在两元组查找的基础上，再添加一层循环。</p><p>也就是一个最初的循环，然后在那个循环里面开始两元组的查找办法。有个需要注意的要点就是：三元组里面可能会出现的重复。</p><p>当我们对初始数据进行预处理（排序）后，相同元素会排在一块。所以需要避免相同三元组出现的话，就需要保证每次遍历的元素都是独一无二的，这是建立在二元组基础上的三元组需要注意的地方。</p><p><a href="https://github.com/aimasa/exercise_demo/tree/master/src/exercise/demo/threesum" target="_blank" rel="noopener">代码示例</a></p><h1 id="四元组相加办法"><a href="#四元组相加办法" class="headerlink" title="四元组相加办法"></a>四元组相加办法</h1><p>四元组又比三元组多了一个元素</p><p>所以又在外层加了一层循环语句，为了让复杂度小于o($n^{3}$)，我们对循环进行了一些操作，又叫剪枝。</p><p>剪枝把可能会产生重复四元组的情况去掉了，也把可能在对四元组的其中一个元素做循环，但没有能和这个元素产生结果的情况去掉了。</p><p>emm，也就是假如是[-1,0,1,2,-1,-4]这个数组我想要的目标值是0，但当我最外层循环指向-4时候，第二层循环，加上第三层循环都不可能能够找到-4和哪三个数相加得到0的时候，我就需要在预判断时候，就能够跳出循环，以避免产生更多的不必要的运算。</p><p>所以，剪枝我的办法是</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>(i &gt; <span class="number">0</span> &amp;&amp; nums[i] == nums[i - <span class="number">1</span>]) &#123;<span class="keyword">continue</span>;&#125;</span><br><span class="line"><span class="keyword">if</span>(nums[i] + nums[i + <span class="number">1</span>] + nums[i + <span class="number">2</span>] + nums[i + <span class="number">3</span>] &gt; target)       </span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span>(nums[i] + nums[n] + nums[n - <span class="number">1</span>] + nums[n - <span class="number">2</span>] &lt; target) </span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">continue</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>预判断最外层指定的元素可不可能是想要的四元组的成员之一。</p><p>或者预判断最外层和次外层指定的两个元素可不可能和另外两个元素产生想要的四元组。</p><p>同时剪枝还需要去重：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>(i &gt; <span class="number">0</span> &amp;&amp; nums[i] == nums[i - <span class="number">1</span>]) &#123;<span class="keyword">continue</span>;&#125;</span><br></pre></td></tr></table></figure><p>判断当前指向的这个元素是不是这个指针上一个地方指向的是同一个元素，如果是，那指向下一个元素。这个去重是三层循环都必须需要的步骤，这样可以安全的去掉可能产生一样的四元组的可能。</p><p><a href="https://github.com/aimasa/exercise_demo/tree/master/src/exercise/demo/foursum" target="_blank" rel="noopener">四元组之和代码示范</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;给定一个包含 n 个整数的数组 nums，判断 nums 中是否存在三个元素 a，b，c ，使得 a + b + c = 0 ？找出所有满足条件且不重复的三元组。&lt;/p&gt;
&lt;p&gt;注意：答案中不可以包含重复的三元组。&lt;/p&gt;
&lt;p&gt;例如, 给定数组 &lt;code&gt;nums = [-1, 0, 1, 2, -1, -4]&lt;/code&gt;，&lt;/p&gt;
&lt;p&gt;满足要求的三元组集合为：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;[&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  [-1, 0, 1],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  [-1, -1, 2]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;]&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;a href=&quot;https://leetcode-cn.com/problems/3sum&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;力扣（LeetCode）&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="算法" scheme="http://aimasa.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
      <category term="三数之和" scheme="http://aimasa.github.io/categories/%E7%AE%97%E6%B3%95/%E4%B8%89%E6%95%B0%E4%B9%8B%E5%92%8C/"/>
    
    
      <category term="算法" scheme="http://aimasa.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>整数反转</title>
    <link href="http://aimasa.github.io/2019/07/18/%E6%95%B4%E6%95%B0%E5%8F%8D%E8%BD%AC/"/>
    <id>http://aimasa.github.io/2019/07/18/整数反转/</id>
    <published>2019-07-18T03:29:49.000Z</published>
    <updated>2019-10-17T01:01:24.099Z</updated>
    
    <content type="html"><![CDATA[<p>给出一个 32 位的有符号整数，你需要将这个整数中每位上的数字进行反转。</p><p>示例 1:</p><pre><code>输入: 123输出: 321</code></pre><p>示例 2:</p><pre><code>输入: -123输出: -321</code></pre><p>示例 3:</p><pre><code>输入: 120输出: 21</code></pre><p>注意:</p><p>假设我们的环境只能存储得下 32 位的有符号整数，则其数值范围为 <code>[−231,  231 − 1]</code>。请根据这个假设，如果反转后整数溢出那么就返回 0。</p><p><a href="https://leetcode-cn.com/problems/reverse-integer" target="_blank" rel="noopener">力扣（LeetCode）</a></p><a id="more"></a><p>这个我先把整数转成<code>string</code>形式，然后再重新把这个字符串拼接，最后输出的字符串转换成整数即可，不过要先转换成<code>long</code>类型整数，再转换成<code>int</code>类型，不然会溢出的。</p><p>官方给的解答是：</p><p>对该整数循环进行整除10，直到最后结果为零才停止循环，在和该数除以10的余数相加，中途记住判断是否溢出即可。</p><p>因为太简单，直接放代码把。</p><p><a href="https://github.com/aimasa/exercise_demo/blob/master/src/exercise/demo/reversea" target="_blank" rel="noopener">代码</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;给出一个 32 位的有符号整数，你需要将这个整数中每位上的数字进行反转。&lt;/p&gt;
&lt;p&gt;示例 1:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;输入: 123
输出: 321
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;示例 2:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;输入: -123
输出: -321
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;示例 3:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;输入: 120
输出: 21
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;注意:&lt;/p&gt;
&lt;p&gt;假设我们的环境只能存储得下 32 位的有符号整数，则其数值范围为 &lt;code&gt;[−231,  231 − 1]&lt;/code&gt;。请根据这个假设，如果反转后整数溢出那么就返回 0。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://leetcode-cn.com/problems/reverse-integer&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;力扣（LeetCode）&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="算法" scheme="http://aimasa.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
      <category term="整数反转" scheme="http://aimasa.github.io/categories/%E7%AE%97%E6%B3%95/%E6%95%B4%E6%95%B0%E5%8F%8D%E8%BD%AC/"/>
    
    
      <category term="算法" scheme="http://aimasa.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>z字形变换</title>
    <link href="http://aimasa.github.io/2019/07/18/z%E5%AD%97%E5%BD%A2%E5%8F%98%E6%8D%A2/"/>
    <id>http://aimasa.github.io/2019/07/18/z字形变换/</id>
    <published>2019-07-18T02:06:03.000Z</published>
    <updated>2019-10-17T01:00:17.851Z</updated>
    
    <content type="html"><![CDATA[<p>将一个给定字符串根据给定的行数，以从上往下、从左到右进行 Z 字形排列。</p><p>比如输入字符串为 “LEETCODEISHIRING” 行数为 3 时，排列如下：</p><pre><code>L   C   I   RE T O E S I I GE   D   H   N</code></pre><p>之后，你的输出需要从左往右逐行读取，产生出一个新的字符串，比如：”LCIRETOESIIGEDHN”。</p><p><a href="https://leetcode-cn.com/problems/zigzag-conversion" target="_blank" rel="noopener">力扣（LeetCode）</a></p><a id="more"></a><p>我用暴力破解，z形输出了输入的字符串，结果发现看错了题目……只是要我z形排序。</p><p>然后官方题解特别简洁</p><p>它先是创建了一个列表，这个列表的长度代表了z字形需要被分成的行数（n = 3时候list.length = 3）</p><p>列表里面装的元素类型是stringBuffer，把每行会被输出的内容拼接在一起，如图：</p><p><center><img src="http://pictures.aimasa.club/static/images/z字形变换/example.png"></center><br><code>flag</code>就是一个布尔（Boolean）值，用来判断z字形输出的走向，应该往上走还是往下走。它控制的是list存储里面应该往上存储还是向下存储，换句话来说就是<code>list&lt;StringBuffer&gt;.get(i)</code>中的<code>i</code>是增加还是还是减少。</p><p>最后再把里面的<code>list</code>里面的<code>stringBuffer</code>循环输出进行拼接，最后返回正确的值。</p><p>所以超简单，但是我就是没想到。</p><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p><a href="https://github.com/aimasa/exercise_demo/tree/tablebookexercise/src/exercise/demo/convert" target="_blank" rel="noopener">代码</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;将一个给定字符串根据给定的行数，以从上往下、从左到右进行 Z 字形排列。&lt;/p&gt;
&lt;p&gt;比如输入字符串为 “LEETCODEISHIRING” 行数为 3 时，排列如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;L   C   I   R
E T O E S I I G
E   D   H   N
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;之后，你的输出需要从左往右逐行读取，产生出一个新的字符串，比如：”LCIRETOESIIGEDHN”。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://leetcode-cn.com/problems/zigzag-conversion&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;力扣（LeetCode）&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="算法" scheme="http://aimasa.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
      <category term="z字形变换" scheme="http://aimasa.github.io/categories/%E7%AE%97%E6%B3%95/z%E5%AD%97%E5%BD%A2%E5%8F%98%E6%8D%A2/"/>
    
    
      <category term="算法" scheme="http://aimasa.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>正则匹配</title>
    <link href="http://aimasa.github.io/2019/07/08/%E6%AD%A3%E5%88%99%E5%8C%B9%E9%85%8D/"/>
    <id>http://aimasa.github.io/2019/07/08/正则匹配/</id>
    <published>2019-07-08T07:38:24.000Z</published>
    <updated>2019-10-17T01:01:44.573Z</updated>
    
    <content type="html"><![CDATA[<p>看代码时候看到正则这块，而且我对它也一直搞不明白，这次干脆写篇博客，让自己对它理解更深一点。</p><a id="more"></a><h1 id="关于对一些正则公式的理解"><a href="#关于对一些正则公式的理解" class="headerlink" title="关于对一些正则公式的理解"></a>关于对一些正则公式的理解</h1><p><code>(?:pattern)</code>:在我参考的博客里面解释的是非获取匹配，也就是只负责匹配，但不会获取匹配结果进行输出，提升了效率</p><p><code>\s</code>：匹配空白字符</p><p><code>\t</code>：制表符</p><p><code>\r</code>：回车符</p><p><code>\n</code>：换行符</p><p><code>[]</code>:括号里面代表或</p><p><code>*</code>:跟在字符后面代表可以有0个或多个这个字符</p><p><code>?</code>：表达前面的表达式可以出线0次或1次</p><p><code>(?&lt;=exp)</code>：匹配最左边的exp表达式能匹配的值。eg:remember (?&lt;=re) 返回的结果就是member</p><h1 id="参考博客"><a href="#参考博客" class="headerlink" title="参考博客"></a>参考博客</h1><p><a href="https://tool.oschina.net/uploads/apidocs/jquery/regexp.html" target="_blank" rel="noopener">关于正则的公式的解析</a></p><p><a href="https://blog.csdn.net/jusang486/article/details/42122837" target="_blank" rel="noopener">关于正则公式的匹配</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;看代码时候看到正则这块，而且我对它也一直搞不明白，这次干脆写篇博客，让自己对它理解更深一点。&lt;/p&gt;
    
    </summary>
    
      <category term="java语法" scheme="http://aimasa.github.io/categories/java%E8%AF%AD%E6%B3%95/"/>
    
      <category term="正则匹配" scheme="http://aimasa.github.io/categories/java%E8%AF%AD%E6%B3%95/%E6%AD%A3%E5%88%99%E5%8C%B9%E9%85%8D/"/>
    
    
      <category term="java语法" scheme="http://aimasa.github.io/tags/java%E8%AF%AD%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>HMM隐马尔可夫模型</title>
    <link href="http://aimasa.github.io/2019/07/03/HMM%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B/"/>
    <id>http://aimasa.github.io/2019/07/03/HMM隐马尔可夫模型/</id>
    <published>2019-07-03T12:02:29.000Z</published>
    <updated>2019-10-17T00:59:50.001Z</updated>
    
    <content type="html"><![CDATA[<p>emm学长让我看看命名实体识别，解释牵涉到HMM隐马尔可夫模型和条件随机场，我先看看，顺便记笔记，写博客。</p><a id="more"></a><h1 id="关于熵（Entropy）"><a href="#关于熵（Entropy）" class="headerlink" title="关于熵（Entropy）"></a>关于熵（Entropy）</h1><p>熵在这里被用来表征系统的无序程度，熵越大，系统越无序。</p><p>负熵是物质系统有序化，组织化，复杂化状态的一种度量。</p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><a href="https://www.cnblogs.com/skyme/p/4651331.html" target="_blank" rel="noopener">隐马尔可夫模型解释</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;emm学长让我看看命名实体识别，解释牵涉到HMM隐马尔可夫模型和条件随机场，我先看看，顺便记笔记，写博客。&lt;/p&gt;
    
    </summary>
    
      <category term="算法" scheme="http://aimasa.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
      <category term="隐马尔可夫模型" scheme="http://aimasa.github.io/categories/%E7%AE%97%E6%B3%95/%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B/"/>
    
    
      <category term="算法" scheme="http://aimasa.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Mockito使用</title>
    <link href="http://aimasa.github.io/2019/06/24/Mockito%E4%BD%BF%E7%94%A8/"/>
    <id>http://aimasa.github.io/2019/06/24/Mockito使用/</id>
    <published>2019-06-24T11:59:21.000Z</published>
    <updated>2019-09-09T06:34:34.908Z</updated>
    
    <content type="html"><![CDATA[<p>关于Mockito的使用，跟着文档边看边试着那些用法。</p><ul><li>来一段从官网搞出来的话</li></ul><blockquote><p>Mockito is a mocking framework that tastes really good. It lets you write beautiful tests with a clean &amp; simple API. Mockito doesn’t give you hangover because the tests are very readable and they produce clean verification errors. </p></blockquote><ul><li>翻译</li></ul><blockquote><p>Mockito是一个体感非常好的mocking框架，它能够让你用简洁干净的API写出漂亮的测试方法……</p></blockquote><p>好了后面的不翻译了，我看了一下，简而言之就是这个测试框架非常好用，特别好用，快来用吧</p><a id="more"></a><h1 id="Mockito使用"><a href="#Mockito使用" class="headerlink" title="Mockito使用"></a>Mockito使用</h1><h2 id="创建mock对象"><a href="#创建mock对象" class="headerlink" title="创建mock对象"></a>创建mock对象</h2><p>刚开始我是<code>mock()</code>一个对象：</p><pre><code>UserPersonService mockedUserPerson = Mockito.mock(UserPersonService.class);</code></pre><p>ps:其中<code>UserPersonService</code>是我想调通的service类。</p><p>然后发现可以用<code>@Mock</code>然后<code>private</code>这个类，然后就很方便的创建了一个Mock对象了：</p><pre><code>@Mockprivate UserPersonService userPersonService;</code></pre><h2 id="验证行为"><a href="#验证行为" class="headerlink" title="验证行为"></a>验证行为</h2><p>这里就是验证方法是否真的被调用，或者调用了多少次。</p><pre><code>List mockedList = mock(List.class);//using mock object 使用mock对象mockedList.add(&quot;one&quot;);mockedList.clear();//verification 验证verify(mockedList).add(&quot;one&quot;);verify(mockedList).clear();</code></pre><p>里面是验证</p><h2 id="做测试桩"><a href="#做测试桩" class="headerlink" title="做测试桩"></a>做测试桩</h2><p>打桩就是对调用的方法给它模拟返回值。然后再后面对这个方法进行调用，输入与之前设定的一样的输入值时候，如果那个方法响应了就会把自己之前模拟的返回值返回出来。这时候我们就能用输出到控制台语句去观摩一下是不是自己预期设定的结果。</p><pre><code>LinkedList mockedList = mock(LinkedList.class); //stubbing // 测试桩 when(mockedList.get(0)).thenReturn(&quot;first&quot;); when(mockedList.get(1)).thenThrow(new RuntimeException());</code></pre><p>这是官网给出来的使用方法，里面当<code>mockedList.get(0)</code>时候返回的是自己设定好的<code>first</code>这个值，然后<code>mockedList.get(1)</code>时候返回的就是自己设定好的抛异常<code>new RuntimeException()</code></p><p>当然，如果自己没有对某个输入参数进行打桩的话，输出就会默认<code>null</code>。</p><p>这个方法也就是看能不能调通这个方法，看是否会响应，会的话就会输出期望值，还能查看这个方法被调用了多少次。</p><h1 id="使用心得"><a href="#使用心得" class="headerlink" title="使用心得"></a>使用心得</h1><p>我试着用<code>@Mock</code>去注解一个类，然后打桩，但是发现打完桩，还是不能返回我打桩时候设定的返回值。</p><p>然后问大佬们，他们给我说用<code>@MockBean</code>，之后就成功了，所以在看<code>@Mock</code>和<code>@MockBean</code>两个注解的区别</p><h2 id="Mock"><a href="#Mock" class="headerlink" title="@Mock"></a>@Mock</h2><p>它们允许模拟类或接口，并记录和验证其上的行为。</p><p>将字段标记为模拟。</p><p>允许速记模拟创建。<br>最小化重复的模拟创建代码。<br>使测试类更具可读性。<br>使验证错误更容易阅读，因为字段名称用于标识模拟。</p><h2 id="MockBean"><a href="#MockBean" class="headerlink" title="@MockBean"></a>@MockBean</h2><p>就是对不需要验证的类加上这个注解，然后它会自动将这个类代入测试的<code>controller</code>中的被<code>@Autowired</code>注解的类。然后返回自己打桩时候设置的值</p><p>Not only will @MockBean provide you with a mock, it will also add that mock as a bean (as the name suggests) within the ApplicationContext, and override any existing beans either by name or by type</p><p>————————<a href="https://gooroo.io/GoorooTHINK/Article/16943/Spring-Boot-14--MockBean-and-SpyBean/24301#.XRQk2egzaM8" target="_blank" rel="noopener">摘自博文</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;关于Mockito的使用，跟着文档边看边试着那些用法。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;来一段从官网搞出来的话&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Mockito is a mocking framework that tastes really good. It lets you write beautiful tests with a clean &amp;amp; simple API. Mockito doesn’t give you hangover because the tests are very readable and they produce clean verification errors. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;翻译&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Mockito是一个体感非常好的mocking框架，它能够让你用简洁干净的API写出漂亮的测试方法……&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;好了后面的不翻译了，我看了一下，简而言之就是这个测试框架非常好用，特别好用，快来用吧&lt;/p&gt;
    
    </summary>
    
      <category term="spring相关" scheme="http://aimasa.github.io/categories/spring%E7%9B%B8%E5%85%B3/"/>
    
      <category term="Mockito使用" scheme="http://aimasa.github.io/categories/spring%E7%9B%B8%E5%85%B3/Mockito%E4%BD%BF%E7%94%A8/"/>
    
    
      <category term="spring相关" scheme="http://aimasa.github.io/tags/spring%E7%9B%B8%E5%85%B3/"/>
    
  </entry>
  
  <entry>
    <title>机器学习之线性回归</title>
    <link href="http://aimasa.github.io/2019/06/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    <id>http://aimasa.github.io/2019/06/21/机器学习之线性回归/</id>
    <published>2019-06-21T01:34:22.000Z</published>
    <updated>2019-10-17T01:01:05.436Z</updated>
    
    <content type="html"><![CDATA[<p>在机器学习中，我听了吴恩达老师的几节课，一节是来介绍机器学习里面的监督学习，监督学习里面以回归分析和分类为代表，然后吴恩达老师介绍了回归算法，在此做笔记。</p><a id="more"></a><p>会用到的表达符号：</p><ul><li>m：培训的样本的数量（prince in 1000’s有47列:m=47）</li></ul><ul><li>x：输入变量，也称为‘特征’(Size in feet^2)</li></ul><ul><li>y：输出变量，也叫‘目标变量’（prince in 1000’s）</li></ul><ul><li>‘#’ ：训练样本的“个数”缩写</li></ul><ul><li>$\theta_{i}\,=\,$：模型参数</li></ul><h1 id="回归算法"><a href="#回归算法" class="headerlink" title="回归算法"></a>回归算法</h1><p>计算函数最小值：求导 = 0（因为求某数导是求的在当前这个数时候的切线斜率，只有在数值达到峰值时候，切线斜率才会为0）</p><p>回归算法分线性回归和非线性回归，这里先讨论线性回归。</p><h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p>就是一条直线，有两个未知数（x和y）</p><p>公式：$h<em>{\theta }(x)=\theta </em>{0}x+\theta_{1}$</p><h2 id="拟合"><a href="#拟合" class="headerlink" title="拟合"></a>拟合</h2><p>拟合就是把平面上的点,用一条光滑的曲线连接起来,因为这条曲线有无数种可能,就有了各种拟合方法.</p><p>拟合误差（即总残差）</p><h2 id="绝对值的导数不存在的推导"><a href="#绝对值的导数不存在的推导" class="headerlink" title="绝对值的导数不存在的推导"></a>绝对值的导数不存在的推导</h2><p>函数可导必要条件是：在此点连续(也就是左连续必须和右连续相同)</p><p>这里是别人对绝对值的导数可能不存在而写出的证明：</p><p><center><img src="http://pictures.aimasa.club/static/images/机器学习之线性回归/proof.png"></center></p><h2 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h2><p><code>最小二乘法</code>是一种数学优化技术，就是可以求到一些未知的数据，并且使这些求得的数据和实际数据间的误差的平方和为最小</p><h2 id="损失函数（loss-function）"><a href="#损失函数（loss-function）" class="headerlink" title="损失函数（loss function）"></a>损失函数（loss function）</h2><p>就是真实值和理论值的偏差。</p><p>因为实际应用中会受到诸多因素的制约</p><h2 id="效用函数"><a href="#效用函数" class="headerlink" title="效用函数"></a>效用函数</h2><p>百度里说：效用函数通常是用来表示消费者在消费中所获得的效用与所消费的商品组合之间数量关系的函数，以衡量消费者从消费既定的商品组合中所获得满足的程度。</p><h2 id="残差平方和"><a href="#残差平方和" class="headerlink" title="残差平方和"></a>残差平方和</h2><p>百度里解释的是：把数据点和它和回归直线上的预估点的差异数据称为残差，然后，所有的残差的平方和被称为残差平方和。它表示随机误差的效应，一组数据的残差平方和越小，它的拟合度也就越好。    </p><h2 id="拟合-1"><a href="#拟合-1" class="headerlink" title="拟合"></a>拟合</h2><p>就是自己通过方程拟出来的曲线和已知的数据相吻合，这个过程叫做拟合</p><h2 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h2><p>在单变量线性回归函数（例）：$h<em>{\theta }(x)=\theta </em>{0}x+\theta<em>{1}$中的$\theta</em>{1}$和$\theta_{0}$(就是模型参数)这两个未知数可以有很多种组合，从而得出不同的线性函数，拟合出不同的曲线。</p><p>但是为了能够让拟合出来的曲线和已知数据尽最大程度吻合，挑选$\theta<em>{1}$和$\theta</em>{0}$很重要。</p><p>所以就有了求平均误差的公式：$J(\theta <em>{0} \,,\, \theta </em>{1} \, )=\frac{1}{2m}\sum<em>{i=1}^{m}(h</em>{\theta }(x^{(i)})-y^{(i)}))^{2}$</p><p>这个公式是用来衡量设置出来的$\theta<em>{1}$和$\theta</em>{0}$的值拟合的曲线和真实的数据的吻合度，所以这个公式得出来的结果越小越好。</p><blockquote><p>其中的$\frac{1}{2}$这是在尝试减小平均误差</p></blockquote><p>于是其中：minimize $J(\theta <em>{0} \,,\, \theta </em>{1} \, )$，就是这个线性函数的代价函数</p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><a href="https://baike.baidu.com/item/拟合" target="_blank" rel="noopener">拟合解释</a></p><p><a href="https://math.stackexchange.com/questions/991475/why-is-the-absolute-value-function-not-differentiable-at-x-0/991559" target="_blank" rel="noopener">绝对值的导数不存在的原因</a></p><p>[参考视频为吴恩达老师的机器学习的入门视频]</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在机器学习中，我听了吴恩达老师的几节课，一节是来介绍机器学习里面的监督学习，监督学习里面以回归分析和分类为代表，然后吴恩达老师介绍了回归算法，在此做笔记。&lt;/p&gt;
    
    </summary>
    
      <category term="算法" scheme="http://aimasa.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
      <category term="机器学习" scheme="http://aimasa.github.io/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="线性回归算法" scheme="http://aimasa.github.io/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="算法" scheme="http://aimasa.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="机器学习" scheme="http://aimasa.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>最长回文串</title>
    <link href="http://aimasa.github.io/2019/06/18/%E6%9C%80%E9%95%BF%E5%9B%9E%E6%96%87%E4%B8%B2/"/>
    <id>http://aimasa.github.io/2019/06/18/最长回文串/</id>
    <published>2019-06-18T12:08:52.000Z</published>
    <updated>2019-10-24T02:45:01.804Z</updated>
    
    <content type="html"><![CDATA[<p>终于弄懂了最长回文串，在做题的时候，总是在暴力破解的思路里出不去，总想着两个for循环，空间复杂度为o($n^{2}$)然后解题思路里面讲了Manacher’s algorithm这个算法，把复杂度降低到了o(n)。</p><a id="more"></a><p>首先看张图（我为了图方便，都先预处理了字符串，不是按官方给的判断回文子串的奇偶方法）</p><p><center><img src="http://pictures.aimasa.club/static/images/最长回文串/time.png"></center><br>上面那行是我用暴力破解用的时间，下面那行是用Manacher’s algorithm这个算法花的时间，可以很明显的看到emm差距。</p><p>（想说一点[题外话]：记得老师说以空间换时间和以时间换空间的这句话，所以我们需要在时间和空间中找到一个平衡点以达到我们想要的效率）</p><h1 id="暴力破解"><a href="#暴力破解" class="headerlink" title="暴力破解"></a>暴力破解</h1><p>这个算法很简单，就是简单粗暴的双循环，接着判断是不是回文串，再判断是不是最长的回文串。</p><p>我用的是先预处理字符串，在字符串中间加入”#”号，字符串首尾也加上这个</p><p>eg：abcd  =====&gt; #a#b#c#d#</p><p>这样就能保证字符串长度一定为奇数了（如果字符串原长3，那么就需要插入4个”#”，那么奇数加偶数一定是奇数了）</p><p>然后根据循环到的位置，扩散开来判断是否是回文串，是的话同时记录长度和下标，就能够寻找到最长的回文串了。</p><h1 id="Manacher’s-algorithm"><a href="#Manacher’s-algorithm" class="headerlink" title="Manacher’s algorithm"></a>Manacher’s algorithm</h1><h2 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h2><p>首先预处理字符串，让字符串保证长度为奇数<br>eg：aba  =====&gt; #a#b#a#</p><h2 id="主要思路"><a href="#主要思路" class="headerlink" title="主要思路"></a>主要思路</h2><p>先假设<strong>当前最长回文串</strong></p><p><center><img src="http://pictures.aimasa.club/static/images/最长回文串/table.png"></center><br>里面的p[i]是指当前回文串的半径（其实p[i] - 1就是当前回文串的真实长度）</p><p>所以，p[j]是最后得出最长回文串的关键。</p><p>所以我们需要求出p[j]（ci的位置就是为p[i]服务的）</p><p>先假设ci是<strong>当前</strong>最长回文串的中心点，那么由它开始往外扩展，判断这个最长回文串里面是否有新的回文串，如果有的话，首先判断新的回文串是不是被这个最大回文串包含了。</p><p>从j=0开始往后遍历，同时开始判断当前最长的回文串，然后ci的位置根据最长回文串的最右边界的位置而开始发生改变。</p><p>接着就要开始讨论关于j的位置处于最长回文串的笼罩范围内的情况了：</p><p><center><img src="http://pictures.aimasa.club/static/images/最长回文串/lenth.png"></center><br>这里列举的是j存在的回文串<strong>处于最长回文串笼罩的范围内的情况</strong>。</p><p>在这里我们没有必要去一个个比较去判断在最长回文串内的i的情况，我们可以直接对j对应ci的另一边的j的映射（我们称为i，i = 2* ci - j）i的p[i]的长度，如果p[i]的回文串长度如上图所示的话，那么p[j]=p[i],然后跳出循环，去遍历下一个j</p><p>除了刚刚那种情况，那就还有剩下的一种情况了：j存在的回文串<strong>长于最长回文串笼罩的范围内的情况</strong></p><p><center><img src="http://pictures.aimasa.club/static/images/最长回文串/overLenth.png"></center><br>其中j的回文子串的右边比ci所对应的回文串的最右边长（如图），那么我们就需要判断一下j对应的i的回文字符串有多长，所以首先我们要知道l的长度，因为对应的i的那个部分是回文字符串，所以在没有超出ci的最长回文串的最右边界的部分，j的那段一定是回文字符串，所以我们就需要在基于l长度的回文字符串的基础下，继续往后比较。</p><p><center><img src="http://pictures.aimasa.club/static/images/最长回文串/final.png"></center><br>如果j的回文串的最右边界超出了ci原本对应的最长回文串的边界</p><p><center><img src="http://pictures.aimasa.club/static/images/最长回文串/realFinal.png"></center><br>那么就令ci的值变成j的值。</p><p>当然，在对ci做改变时候，要记住随时记录p[j]的值，并且随时记录p[j]里面的最大值。</p><h2 id="结尾"><a href="#结尾" class="headerlink" title="结尾"></a>结尾</h2><p>所以最后就根据p[j]的最大值找到对应的字符子串，这个子串就是最大回文串</p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><a href="https://leetcode-cn.com/problems/longest-palindromic-substring/solution/zhong-xin-kuo-san-dong-tai-gui-hua-by-liweiwei1419/" target="_blank" rel="noopener">参考官方资料</a></p><p><a href="https://github.com/aimasa/exercise_demo/tree/master/src/exercise/demo/longestpalindrome" target="_blank" rel="noopener">本文代码</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;终于弄懂了最长回文串，在做题的时候，总是在暴力破解的思路里出不去，总想着两个for循环，空间复杂度为o($n^{2}$)然后解题思路里面讲了Manacher’s algorithm这个算法，把复杂度降低到了o(n)。&lt;/p&gt;
    
    </summary>
    
      <category term="算法" scheme="http://aimasa.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
      <category term="最长回文串" scheme="http://aimasa.github.io/categories/%E7%AE%97%E6%B3%95/%E6%9C%80%E9%95%BF%E5%9B%9E%E6%96%87%E4%B8%B2/"/>
    
    
      <category term="算法" scheme="http://aimasa.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
</feed>
